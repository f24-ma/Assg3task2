{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dILYGMYBo0hT"
      },
      "source": [
        " **TASK 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt8nSQ8VCEUO",
        "outputId": "9bfd2643-3348-44a7-8c86-e96567f1dc80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.51.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.47)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install pandas tqdm streamlit langchain chromadb python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dNTPkC_M1Ta",
        "outputId": "8021bab0-015c-4fe8-d80d-b21021edd370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "-rw------- 1 root root 17M Nov 29 22:16 /content/mtsamples.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/MyDrive/mtsamples.csv\" \"/content/mtsamples.csv\"\n",
        "!ls -lh /content/mtsamples.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0wE1g_POaGY",
        "outputId": "9a23cca9-05ce-44dc-fa09-15354168b616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded: (4999, 6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4999/4999 [00:01<00:00, 3416.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks saved: 14026\n",
            "Saved file: /content/chunks/chunks.jsonl\n",
            "\n",
            "Sample chunk:\n",
            "doc0_chunk0\n",
            "SUBJECTIVE:, This 23-year-old white female presents with complaint of allergies. She used to have allergies when she lived in Seattle but she thinks they are worse here. In the past, she has tried Claritin, and Zyrtec. Both worked for short time but then seemed to lose effectiveness. She has used Allegra also. She used that last summer and she began using it again two weeks ago. It does not appear to be working very well. She has used over-the-counter sprays but no prescription nasal sprays. She\n"
          ]
        }
      ],
      "source": [
        "#cleaning and chunking\n",
        "import pandas as pd\n",
        "import json, re, os\n",
        "from tqdm import tqdm\n",
        "\n",
        "csv_file = \"/content/mtsamples.csv\"\n",
        "\n",
        "os.makedirs(\"/content/chunks\", exist_ok=True)\n",
        "out_file = \"/content/chunks/chunks.jsonl\"\n",
        "\n",
        "df = pd.read_csv(csv_file)\n",
        "print(\"Data loaded:\", df.shape)\n",
        "\n",
        "text_col = \"transcription\"\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.strip()                 # extra spaces remove\n",
        "    text = re.sub(r\"\\s+\", \" \", text)    # multiple spaces\n",
        "    return text\n",
        "\n",
        "# chunks\n",
        "def make_chunks(text, size=1500, overlap=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + size\n",
        "        piece = text[start:end].strip()\n",
        "        if piece:\n",
        "            chunks.append(piece)\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "\n",
        "all_chunks = []\n",
        "chunk_id = 0\n",
        "\n",
        "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    raw_text = row[text_col]\n",
        "    clean = clean_text(raw_text)\n",
        "\n",
        "    if clean == \"\":\n",
        "        continue\n",
        "\n",
        "    for c in make_chunks(clean):\n",
        "        all_chunks.append({\n",
        "            \"chunk_id\": f\"doc{i}_chunk{chunk_id}\",\n",
        "            \"source_doc\": f\"doc{i}\",\n",
        "            \"text\": c,\n",
        "            \"meta\": {\n",
        "                \"sample_name\": row.get(\"sample_name\", \"\"),\n",
        "                \"medical_specialty\": row.get(\"medical_specialty\", \"\")\n",
        "            }\n",
        "        })\n",
        "        chunk_id += 1\n",
        "\n",
        "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for ch in all_chunks:\n",
        "        f.write(json.dumps(ch) + \"\\n\")\n",
        "\n",
        "print(\"Total chunks saved:\", len(all_chunks))\n",
        "print(\"Saved file:\", out_file)\n",
        "\n",
        "print(\"\\nSample chunk:\")\n",
        "print(all_chunks[0][\"chunk_id\"])\n",
        "print(all_chunks[0][\"text\"][:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYkI74iIOa6G",
        "outputId": "99a722ca-5a00-46ec-d18c-3e64ad650da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks to embed: 14026\n",
            "Embedding dim: 384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/1754 [00:00<03:15,  8.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 206/1754 [00:09<01:09, 22.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 1600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 409/1754 [00:15<00:37, 35.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 3200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 608/1754 [00:19<00:32, 34.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 4800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 806/1754 [00:25<00:34, 27.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 6400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 1007/1754 [00:30<00:21, 35.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 1207/1754 [00:35<00:15, 34.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 9600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 1409/1754 [00:41<00:10, 31.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 11200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 1609/1754 [00:46<00:04, 33.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at batch 12800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1754/1754 [00:49<00:00, 35.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Index and metadata in Drive: /content/drive/MyDrive/medical_rag_vectorstore\n"
          ]
        }
      ],
      "source": [
        "#  Embedding and FAISS\n",
        "!pip install -q sentence-transformers faiss-cpu\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, json, os, numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "CHUNKS_FILE = \"/content/chunks/chunks.jsonl\"\n",
        "DRIVE_SAVE = \"/content/drive/MyDrive/medical_rag_vectorstore\"\n",
        "os.makedirs(DRIVE_SAVE, exist_ok=True)\n",
        "\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "BATCH = 8\n",
        "\n",
        "#  load the chunks texts\n",
        "texts = []\n",
        "metas = []\n",
        "with open(CHUNKS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        r = json.loads(line)\n",
        "        texts.append(r[\"text\"])\n",
        "        metas.append({\"chunk_id\": r[\"chunk_id\"], \"source_doc\": r[\"source_doc\"], **r.get(\"meta\", {})})\n",
        "\n",
        "print(\"Chunks to embed:\", len(texts))\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "dim = model.encode([\"hi\"]).shape[1]\n",
        "print(\"Embedding dim:\", dim)\n",
        "\n",
        "#   faiss index and add the  embeddings in the batches\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "for i in tqdm(range(0, len(texts), BATCH)):\n",
        "    batch = texts[i:i+BATCH]\n",
        "    emb = model.encode(batch, show_progress_bar=False).astype(\"float32\")\n",
        "    index.add(emb)\n",
        "    # checkpoint of every 200 batches\n",
        "    if (i // BATCH) % 200 == 0:\n",
        "        faiss.write_index(index, \"/content/temp_faiss.index\")\n",
        "        !cp /content/temp_faiss.index \"{DRIVE_SAVE}/faiss.index\"\n",
        "        print(\"Checkpoint saved at batch\", i)\n",
        "\n",
        "faiss.write_index(index, \"/content/temp_faiss.index\")\n",
        "!cp /content/temp_faiss.index \"{DRIVE_SAVE}/faiss.index\"\n",
        "\n",
        "with open(os.path.join(DRIVE_SAVE, \"metadata.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for m in metas:\n",
        "        f.write(json.dumps(m, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Done. Index and metadata in Drive:\", DRIVE_SAVE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZa-VzdKdFuu",
        "outputId": "3d156397-55b2-4076-b61d-8ae7bddcde23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'chunk_id': 'doc3347_chunk9226', 'source_doc': 'doc3347', 'dist': 0.8792160153388977}, {'chunk_id': 'doc4384_chunk12171', 'source_doc': 'doc4384', 'dist': 0.8792160153388977}, {'chunk_id': 'doc3364_chunk9277', 'source_doc': 'doc3364', 'dist': 0.9737875461578369}, {'chunk_id': 'doc4402_chunk12224', 'source_doc': 'doc4402', 'dist': 0.9737875461578369}, {'chunk_id': 'doc1930_chunk5160', 'source_doc': 'doc1930', 'dist': 0.9869530200958252}, {'chunk_id': 'doc3374_chunk9304', 'source_doc': 'doc3374', 'dist': 0.9869530200958252}, {'chunk_id': 'doc3848_chunk10543', 'source_doc': 'doc3848', 'dist': 0.9869530200958252}, {'chunk_id': 'doc4422_chunk12280', 'source_doc': 'doc4422', 'dist': 0.9869530200958252}, {'chunk_id': 'doc3721_chunk10165', 'source_doc': 'doc3721', 'dist': 0.9901379942893982}, {'chunk_id': 'doc4143_chunk11353', 'source_doc': 'doc4143', 'dist': 0.9901379942893982}]\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, json, numpy as np\n",
        "\n",
        "DRIVE_INDEX = \"/content/drive/MyDrive/medical_rag_vectorstore/faiss.index\"\n",
        "DRIVE_META  = \"/content/drive/MyDrive/medical_rag_vectorstore/metadata.jsonl\"\n",
        "MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "index = faiss.read_index(DRIVE_INDEX)\n",
        "metas = [json.loads(l) for l in open(DRIVE_META, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "def search(query, k=10):\n",
        "    qv = MODEL.encode([query]).astype(\"float32\")\n",
        "    D, I = index.search(qv, k)\n",
        "    out = []\n",
        "    for idx, dist in zip(I[0], D[0]):\n",
        "        m = metas[idx]\n",
        "        out.append({\"chunk_id\": m[\"chunk_id\"], \"source_doc\": m.get(\"source_doc\"), \"dist\": float(dist)})\n",
        "    return out\n",
        "\n",
        "# example\n",
        "print(search(\"symptoms of allergic rhinitis\", k=10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT58Ir4udQIW",
        "outputId": "64164439-45ed-4e3e-e001-eaaa7db0ca91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chief Complaint:, coughing up blood and severe joint pain.,History of Present Illness:, The patient is a 37 year old African American woman with history of chronic allergic rhinitis who presents to an outpatient clinic with severe pain in multiple joints and hemoptysis for 1 day. The patient was at her baseline state of health until 2 months prior to admission when her usual symptoms of allergic rhinitis worsened. In addition to increased nasal congestion and drainage, she also began having generalized fatigue, malaise, and migratory arthralgias involving bilateral wrists, shoulders, elbows, knees, ankles, and finger joints. She also had intermittent episodes of swollen fingers that prevented her from making a fist. Patient denied recent flu-like illness, fever, chills, myalgias, or night sweats. Four weeks after the onset of arthralgias patient developed severe bilateral eye dryness and\n",
            "\n",
            "SOURCES: doc3347_chunk9226;doc4384_chunk12171;doc1930_chunk5160\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "#  read a chunk text by id\n",
        "def get_chunk_text(chunk_id):\n",
        "    with open(\"/content/chunks/chunks.jsonl\",\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            r = json.loads(line)\n",
        "            if r[\"chunk_id\"] == chunk_id:\n",
        "                return r[\"text\"]\n",
        "    return \"\"\n",
        "\n",
        "def quick_answer(query, k=3):\n",
        "    res = search(query, k=k)\n",
        "    if not res: return \"No results.\"\n",
        "    top = res[0][\"chunk_id\"]\n",
        "    text = get_chunk_text(top)\n",
        "    return text[:900] + \"\\n\\nSOURCES: \" + \";\".join([r[\"chunk_id\"] for r in res])\n",
        "\n",
        "print(quick_answer(\"what are common symptoms of allergic rhinitis?\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSYPQLeRdUQW"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mqF-GSldg1j",
        "outputId": "aa2c607f-e43b-4245-de1d-18da0040a934"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/eval/test_queries.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/eval/test_queries.csv\n",
        "What are common symptoms of pneumonia?\n",
        "What is the recommended initial treatment for acute bronchitis?\n",
        "What are signs of myocardial infarction in adults?\n",
        "How do you manage type II diabetes?\n",
        "What are typical presentation features of meningitis?\n",
        "What are red flags for abdominal pain?\n",
        "What are first-line antibiotics for uncomplicated UTI?\n",
        "What are indications for CT scan in head trauma?\n",
        "What is the vaccination schedule for tetanus?\n",
        "What are symptoms of sepsis?\n",
        "How to treat allergic rhinitis?\n",
        "How to manage hypertension in elderly patients?\n",
        "What is DKA and how does it present?\n",
        "How to manage postpartum hemorrhage?\n",
        "What are signs of stroke and immediate actions?\n",
        "What are typical features of COPD exacerbation?\n",
        "When is a biopsy indicated for a skin lesion?\n",
        "What are symptoms of appendicitis?\n",
        "How is anemia of chronic disease diagnosed?\n",
        "How to counsel on smoking cessation?\n",
        "What are pediatric fever red flags?\n",
        "What is the treatment for uncomplicated otitis media?\n",
        "What are indications for MRI in back pain?\n",
        "What is standard dosing of acetaminophen in adults?\n",
        "What are contraindications for MRI?\n",
        "How to interpret basic ABG values?\n",
        "What are symptoms of hyperthyroidism?\n",
        "How to manage moderate dehydration?\n",
        "When to refer to a specialist for chest pain?\n",
        "Are antibiotics recommended for uncomplicated URI?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuOSZlWqeF9_",
        "outputId": "1c999cfc-1a67-41df-a465-5f1abda2bf33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/src/run_eval.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/src/run_eval.py\n",
        "import csv\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "#  vectorstore loaded\n",
        "index = faiss.read_index(\"/content/drive/MyDrive/medical_rag_vectorstore/faiss.index\")\n",
        "meta = [json.loads(l) for l in open(\"/content/drive/MyDrive/medical_rag_vectorstore/metadata.jsonl\")]\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def search(q, k=3):\n",
        "    qv = model.encode([q]).astype(\"float32\")\n",
        "    D, I = index.search(qv, k)\n",
        "    return I[0]\n",
        "\n",
        "def get_chunk(cid):\n",
        "    with open(\"/content/chunks/chunks.jsonl\",\"r\") as f:\n",
        "        for line in f:\n",
        "            r=json.loads(line)\n",
        "            if r[\"chunk_id\"]==cid:\n",
        "                return r[\"text\"]\n",
        "    return \"\"\n",
        "\n",
        "def answer(q):\n",
        "    top_ids = search(q)\n",
        "    cid = meta[top_ids[0]][\"chunk_id\"]\n",
        "    text = get_chunk(cid)\n",
        "    return text[:800], cid\n",
        "\n",
        "with open(\"/content/eval/test_queries.csv\") as f:\n",
        "    queries = [l.strip() for l in f if l.strip()]\n",
        "\n",
        "with open(\"/content/drive/MyDrive/eval_results_final.csv\",\"w\",newline='') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow([\"question\",\"answer\",\"source\",\"correct(1/0)\"])\n",
        "    for q in queries:\n",
        "        ans, src = answer(q)\n",
        "        w.writerow([q, ans.replace(\"\\n\",\" \"), src, \"\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpCsKsY1eJAU",
        "outputId": "aff93bbc-dbf5-4d7b-ecd7-221005e5735d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-29 22:18:17.407167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764454697.426697   35524 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764454697.432557   35524 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764454697.448289   35524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764454697.448310   35524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764454697.448314   35524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764454697.448317   35524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/src/run_eval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xX5m6dceTUq",
        "outputId": "9ef66165-f6f3-49a3-fd28-b1b78e787b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sys.path updated\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content')\n",
        "sys.path.append('/content/src')\n",
        "print(\"sys.path updated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okUzadnTfnjW",
        "outputId": "d928ed59-b481-4c00-d13c-65527834d6c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_queries.csv created at /content/eval/test_queries.csv\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 1301 Nov 29 22:18 test_queries.csv\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "cat > /content/eval/test_queries.csv <<'CSV'\n",
        "What are common symptoms of pneumonia?\n",
        "What is the recommended initial treatment for acute bronchitis?\n",
        "What are signs of myocardial infarction in adults?\n",
        "How do you manage type II diabetes?\n",
        "What are typical presentation features of meningitis?\n",
        "What are red flags for abdominal pain?\n",
        "What are first-line antibiotics for uncomplicated UTI?\n",
        "What are indications for CT scan in head trauma?\n",
        "What is the vaccination schedule for tetanus?\n",
        "What are symptoms of sepsis?\n",
        "How to treat allergic rhinitis?\n",
        "How to manage hypertension in elderly patients?\n",
        "What is DKA and how does it present?\n",
        "How to manage postpartum hemorrhage?\n",
        "What are signs of stroke and immediate actions?\n",
        "What are typical features of COPD exacerbation?\n",
        "When is a biopsy indicated for a skin lesion?\n",
        "What are symptoms of appendicitis?\n",
        "How is anemia of chronic disease diagnosed?\n",
        "How to counsel on smoking cessation?\n",
        "What are pediatric fever red flags?\n",
        "What is the treatment for uncomplicated otitis media?\n",
        "What are indications for MRI in back pain?\n",
        "What is standard dosing of acetaminophen in adults?\n",
        "What are contraindications for MRI?\n",
        "How to interpret basic ABG values?\n",
        "What are symptoms of hyperthyroidism?\n",
        "How to manage moderate dehydration?\n",
        "When to refer to a specialist for chest pain?\n",
        "Are antibiotics recommended for uncomplicated URI?\n",
        "CSV\n",
        "\n",
        "echo \"test_queries.csv created at /content/eval/test_queries.csv\"\n",
        "ls -l /content/eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdyBoARefqrH",
        "outputId": "8b2073c3-0c12-4a1b-dce7-555ba9b02b42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/src/run_eval.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/src/run_eval.py\n",
        "import csv, json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, numpy as np\n",
        "\n",
        "INDEX_PATH = \"/content/drive/MyDrive/medical_rag_vectorstore/faiss.index\"\n",
        "META_PATH = \"/content/drive/MyDrive/medical_rag_vectorstore/metadata.jsonl\"\n",
        "CHUNKS_FILE = \"/content/chunks/chunks.jsonl\"\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "index = faiss.read_index(INDEX_PATH)\n",
        "metas = [json.loads(l) for l in open(META_PATH, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "def search(query, k=3):\n",
        "    qv = model.encode([query]).astype(\"float32\")\n",
        "    D, I = index.search(qv, k)\n",
        "    return I[0], D[0]\n",
        "\n",
        "def get_chunk_text(chunk_id):\n",
        "    with open(CHUNKS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            r = json.loads(line)\n",
        "            if r[\"chunk_id\"] == chunk_id:\n",
        "                return r[\"text\"]\n",
        "    return \"\"\n",
        "\n",
        "with open(\"/content/eval/test_queries.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    queries = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "out_path = \"/content/drive/MyDrive/eval_results_final.csv\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\", newline='') as fout:\n",
        "    writer = csv.writer(fout)\n",
        "    writer.writerow([\"question\",\"answer\",\"sources\",\"correct(1/0)\"])\n",
        "    for q in queries:\n",
        "        idxs, dists = search(q, k=3)\n",
        "        sources = []\n",
        "        for i in idxs:\n",
        "            sources.append(metas[i][\"chunk_id\"])\n",
        "        top_cid = sources[0] if sources else \"\"\n",
        "        ans_text = get_chunk_text(top_cid)[:900].replace(\"\\n\",\" \")\n",
        "        writer.writerow([q, ans_text, \";\".join(sources), \"\"])\n",
        "print(\"Eval results saved ->\", out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3b-j4NUgGsR",
        "outputId": "9d7e25da-e91a-4d49-efbf-a658b69a4071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/src/run_eval_final.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/src/run_eval_final.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QilzaG42cMTR",
        "outputId": "55dae33f-19cf-435f-f193-77fcc3a4794c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 1301 Nov 29 22:18 test_queries.csv\n"
          ]
        }
      ],
      "source": [
        "!ls -l /content/eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j5grzPHTwb-U",
        "outputId": "b4833fd0-2ab5-4730-c059-c188352057d6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"How to manage moderate dehydration?\",\n          \"What are typical features of COPD exacerbation?\",\n          \"What is standard dosing of acetaminophen in adults?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"dependent with respiratory acidosis.,2. Septicemia, septic shock secondary to cellulitis of the leg.,3. Acute renal shutdown.,4. Elevated cardiac enzyme profile without prior cardiac history possibly due to sepsis and also acute renal failure.,RECOMMENDATIONS:,1. Echocardiogram to assess LV function to rule out any cardiac valvular involvement.,2. Aggressive medical management including dialysis.,3. From cardiac standpoint, conservative treatment at this juncture. His cardiac enzyme profile could be elevated secondary to sepsis and also underlying renal failure.,4. Explained to patient's family in detail regarding condition which is critical which they are aware of.\",\n          \"e that she is not hyperthyroid.\",\n          \"ng is applied with Neosporin ointment. The patient is given Tylenol No. 3, tabs #4, to take home with him and take one or two every four hours p.r.n. for pain. He is to return tomorrow for a dressing change. Tetanus immunization is up to date. Preprinted instructions are given. Workers' Compensation first report and work status report are completed.,DISPOSITION: , Home.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sources\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"doc4442_chunk12348;doc4817_chunk13495;doc1393_chunk3859\",\n          \"doc3790_chunk10344;doc4094_chunk11215;doc3790_chunk10343\",\n          \"doc3107_chunk8592;doc3441_chunk9500;doc4020_chunk10993\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct(1/0)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a885c0b0-657e-4e98-8330-a6878989b39b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>sources</th>\n",
              "      <th>correct(1/0)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are common symptoms of pneumonia?</td>\n",
              "      <td>rhythm. Normal S1, S2.,Abdomen: Positive bowel...</td>\n",
              "      <td>doc3369_chunk9290;doc4401_chunk12223;doc3856_c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the recommended initial treatment for ...</td>\n",
              "      <td>ds, and 70% monocytes. A urinalysis obtained i...</td>\n",
              "      <td>doc3865_chunk10602;doc4954_chunk13858;doc1133_...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are signs of myocardial infarction in adu...</td>\n",
              "      <td>ANGINA, is chest pain due to a lack of oxygen ...</td>\n",
              "      <td>doc4971_chunk13921;doc3430_chunk9462;doc4502_c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do you manage type II diabetes?</td>\n",
              "      <td>SUBJECTIVE:, The patient is a 79-year-old Afri...</td>\n",
              "      <td>doc1415_chunk3900;doc3995_chunk10928;doc4459_c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are typical presentation features of meni...</td>\n",
              "      <td>chronic.,There is a joint effusion. There is s...</td>\n",
              "      <td>doc1572_chunk4218;doc2134_chunk5711;doc2467_ch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What are red flags for abdominal pain?</td>\n",
              "      <td>REASON FOR EXAM: , Right-sided abdominal pain ...</td>\n",
              "      <td>doc1709_chunk4498;doc3035_chunk8352;doc3603_ch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What are first-line antibiotics for uncomplica...</td>\n",
              "      <td>cessation of the antibiotic.</td>\n",
              "      <td>doc1175_chunk3264;doc3777_chunk10313;doc129_ch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What are indications for CT scan in head trauma?</td>\n",
              "      <td>EXAM: , CT head without contrast.,INDICATIONS:...</td>\n",
              "      <td>doc1690_chunk4460;doc2922_chunk8027;doc1688_ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What is the vaccination schedule for tetanus?</td>\n",
              "      <td>ng is applied with Neosporin ointment. The pat...</td>\n",
              "      <td>doc3107_chunk8592;doc3441_chunk9500;doc4020_ch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are symptoms of sepsis?</td>\n",
              "      <td>dependent with respiratory acidosis.,2. Septic...</td>\n",
              "      <td>doc4442_chunk12348;doc4817_chunk13495;doc1393_...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How to treat allergic rhinitis?</td>\n",
              "      <td>Discussed vasomotor rhinitis. I suggested she ...</td>\n",
              "      <td>doc2555_chunk6909;doc4181_chunk11494;doc3323_c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How to manage hypertension in elderly patients?</td>\n",
              "      <td>SUBJECTIVE:, The patient is an 89-year-old lad...</td>\n",
              "      <td>doc1375_chunk3815;doc3304_chunk9092;doc3383_ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>What is DKA and how does it present?</td>\n",
              "      <td>d as cerebral edema developed. She was pronoun...</td>\n",
              "      <td>doc1702_chunk4486;doc2935_chunk8054;doc1225_ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>How to manage postpartum hemorrhage?</td>\n",
              "      <td>here was normal placenta, normal pelvic anatom...</td>\n",
              "      <td>doc2593_chunk7021;doc3917_chunk10733;doc673_ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>What are signs of stroke and immediate actions?</td>\n",
              "      <td>turned; then waxed and waned. There was no rep...</td>\n",
              "      <td>doc1700_chunk4478;doc2927_chunk8033;doc2862_ch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>What are typical features of COPD exacerbation?</td>\n",
              "      <td>use.,REVIEW OF SYSTEMS: , The patient has no n...</td>\n",
              "      <td>doc1327_chunk3716;doc3233_chunk8909;doc3813_ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>When is a biopsy indicated for a skin lesion?</td>\n",
              "      <td>PREOPERATIVE DIAGNOSIS:, Worrisome skin lesion...</td>\n",
              "      <td>doc397_chunk1072;doc4006_chunk10947;doc338_chu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>What are symptoms of appendicitis?</td>\n",
              "      <td>REASON FOR EXAM: , Right-sided abdominal pain ...</td>\n",
              "      <td>doc1709_chunk4498;doc3035_chunk8352;doc3603_ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>How is anemia of chronic disease diagnosed?</td>\n",
              "      <td>DIAGNOSIS:, Refractory anemia that is transfus...</td>\n",
              "      <td>doc3198_chunk8821;doc3450_chunk9530;doc4580_ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>How to counsel on smoking cessation?</td>\n",
              "      <td>nt stop smoking.,After her bedrest is complete...</td>\n",
              "      <td>doc1099_chunk3041;doc4916_chunk13756;doc175_ch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>What are pediatric fever red flags?</td>\n",
              "      <td>CHIEF COMPLAINT:, Irritable baby with fever fo...</td>\n",
              "      <td>doc1931_chunk5162;doc4345_chunk12041;doc1938_c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>What is the treatment for uncomplicated otitis...</td>\n",
              "      <td>nction, chronic otitis media with effusion, re...</td>\n",
              "      <td>doc3754_chunk10255;doc4438_chunk12335;doc3729_...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>What are indications for MRI in back pain?</td>\n",
              "      <td>REASON FOR CONSULT:, Depression.,HPI:, The pat...</td>\n",
              "      <td>doc1786_chunk4697;doc4168_chunk11440;doc1864_c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>What is standard dosing of acetaminophen in ad...</td>\n",
              "      <td>units per day, Metamucil p.r.n., enteric-coate...</td>\n",
              "      <td>doc2796_chunk7661;doc4252_chunk11699;doc3321_c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>What are contraindications for MRI?</td>\n",
              "      <td>EXAM:, MRI Head W&amp;WO Contrast.,REASON FOR EXAM...</td>\n",
              "      <td>doc1575_chunk4222;doc2821_chunk7744;doc2761_ch...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>How to interpret basic ABG values?</td>\n",
              "      <td>multiple values for this test, this must be BMP.,</td>\n",
              "      <td>doc3091_chunk8501;doc3091_chunk8500;doc777_chu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>What are symptoms of hyperthyroidism?</td>\n",
              "      <td>e that she is not hyperthyroid.</td>\n",
              "      <td>doc3790_chunk10344;doc4094_chunk11215;doc3790_...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>How to manage moderate dehydration?</td>\n",
              "      <td>drate load consumed. Recommend the patient eit...</td>\n",
              "      <td>doc1442_chunk3970;doc4000_chunk10937;doc4543_c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>When to refer to a specialist for chest pain?</td>\n",
              "      <td>INDICATIONS: ,Chest pain.,STRESS TECHNIQUE:,</td>\n",
              "      <td>doc1522_chunk4127;doc4659_chunk13057;doc4544_c...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Are antibiotics recommended for uncomplicated ...</td>\n",
              "      <td>cessation of the antibiotic.</td>\n",
              "      <td>doc1175_chunk3264;doc3777_chunk10313;doc129_ch...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a885c0b0-657e-4e98-8330-a6878989b39b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a885c0b0-657e-4e98-8330-a6878989b39b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a885c0b0-657e-4e98-8330-a6878989b39b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c3a7cbd4-3a4a-4b10-9ed2-50f8aab48cee\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c3a7cbd4-3a4a-4b10-9ed2-50f8aab48cee')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c3a7cbd4-3a4a-4b10-9ed2-50f8aab48cee button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             question  \\\n",
              "0              What are common symptoms of pneumonia?   \n",
              "1   What is the recommended initial treatment for ...   \n",
              "2   What are signs of myocardial infarction in adu...   \n",
              "3                 How do you manage type II diabetes?   \n",
              "4   What are typical presentation features of meni...   \n",
              "5              What are red flags for abdominal pain?   \n",
              "6   What are first-line antibiotics for uncomplica...   \n",
              "7    What are indications for CT scan in head trauma?   \n",
              "8       What is the vaccination schedule for tetanus?   \n",
              "9                        What are symptoms of sepsis?   \n",
              "10                    How to treat allergic rhinitis?   \n",
              "11    How to manage hypertension in elderly patients?   \n",
              "12               What is DKA and how does it present?   \n",
              "13               How to manage postpartum hemorrhage?   \n",
              "14    What are signs of stroke and immediate actions?   \n",
              "15    What are typical features of COPD exacerbation?   \n",
              "16      When is a biopsy indicated for a skin lesion?   \n",
              "17                 What are symptoms of appendicitis?   \n",
              "18        How is anemia of chronic disease diagnosed?   \n",
              "19               How to counsel on smoking cessation?   \n",
              "20                What are pediatric fever red flags?   \n",
              "21  What is the treatment for uncomplicated otitis...   \n",
              "22         What are indications for MRI in back pain?   \n",
              "23  What is standard dosing of acetaminophen in ad...   \n",
              "24                What are contraindications for MRI?   \n",
              "25                 How to interpret basic ABG values?   \n",
              "26              What are symptoms of hyperthyroidism?   \n",
              "27                How to manage moderate dehydration?   \n",
              "28      When to refer to a specialist for chest pain?   \n",
              "29  Are antibiotics recommended for uncomplicated ...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   rhythm. Normal S1, S2.,Abdomen: Positive bowel...   \n",
              "1   ds, and 70% monocytes. A urinalysis obtained i...   \n",
              "2   ANGINA, is chest pain due to a lack of oxygen ...   \n",
              "3   SUBJECTIVE:, The patient is a 79-year-old Afri...   \n",
              "4   chronic.,There is a joint effusion. There is s...   \n",
              "5   REASON FOR EXAM: , Right-sided abdominal pain ...   \n",
              "6                        cessation of the antibiotic.   \n",
              "7   EXAM: , CT head without contrast.,INDICATIONS:...   \n",
              "8   ng is applied with Neosporin ointment. The pat...   \n",
              "9   dependent with respiratory acidosis.,2. Septic...   \n",
              "10  Discussed vasomotor rhinitis. I suggested she ...   \n",
              "11  SUBJECTIVE:, The patient is an 89-year-old lad...   \n",
              "12  d as cerebral edema developed. She was pronoun...   \n",
              "13  here was normal placenta, normal pelvic anatom...   \n",
              "14  turned; then waxed and waned. There was no rep...   \n",
              "15  use.,REVIEW OF SYSTEMS: , The patient has no n...   \n",
              "16  PREOPERATIVE DIAGNOSIS:, Worrisome skin lesion...   \n",
              "17  REASON FOR EXAM: , Right-sided abdominal pain ...   \n",
              "18  DIAGNOSIS:, Refractory anemia that is transfus...   \n",
              "19  nt stop smoking.,After her bedrest is complete...   \n",
              "20  CHIEF COMPLAINT:, Irritable baby with fever fo...   \n",
              "21  nction, chronic otitis media with effusion, re...   \n",
              "22  REASON FOR CONSULT:, Depression.,HPI:, The pat...   \n",
              "23  units per day, Metamucil p.r.n., enteric-coate...   \n",
              "24  EXAM:, MRI Head W&WO Contrast.,REASON FOR EXAM...   \n",
              "25  multiple values for this test, this must be BMP.,   \n",
              "26                    e that she is not hyperthyroid.   \n",
              "27  drate load consumed. Recommend the patient eit...   \n",
              "28       INDICATIONS: ,Chest pain.,STRESS TECHNIQUE:,   \n",
              "29                       cessation of the antibiotic.   \n",
              "\n",
              "                                              sources  correct(1/0)  \n",
              "0   doc3369_chunk9290;doc4401_chunk12223;doc3856_c...             0  \n",
              "1   doc3865_chunk10602;doc4954_chunk13858;doc1133_...             0  \n",
              "2   doc4971_chunk13921;doc3430_chunk9462;doc4502_c...             1  \n",
              "3   doc1415_chunk3900;doc3995_chunk10928;doc4459_c...             1  \n",
              "4   doc1572_chunk4218;doc2134_chunk5711;doc2467_ch...             0  \n",
              "5   doc1709_chunk4498;doc3035_chunk8352;doc3603_ch...             0  \n",
              "6   doc1175_chunk3264;doc3777_chunk10313;doc129_ch...             0  \n",
              "7   doc1690_chunk4460;doc2922_chunk8027;doc1688_ch...             1  \n",
              "8   doc3107_chunk8592;doc3441_chunk9500;doc4020_ch...             0  \n",
              "9   doc4442_chunk12348;doc4817_chunk13495;doc1393_...             1  \n",
              "10  doc2555_chunk6909;doc4181_chunk11494;doc3323_c...             1  \n",
              "11  doc1375_chunk3815;doc3304_chunk9092;doc3383_ch...             1  \n",
              "12  doc1702_chunk4486;doc2935_chunk8054;doc1225_ch...             1  \n",
              "13  doc2593_chunk7021;doc3917_chunk10733;doc673_ch...             1  \n",
              "14  doc1700_chunk4478;doc2927_chunk8033;doc2862_ch...             0  \n",
              "15  doc1327_chunk3716;doc3233_chunk8909;doc3813_ch...             1  \n",
              "16  doc397_chunk1072;doc4006_chunk10947;doc338_chu...             1  \n",
              "17  doc1709_chunk4498;doc3035_chunk8352;doc3603_ch...             1  \n",
              "18  doc3198_chunk8821;doc3450_chunk9530;doc4580_ch...             1  \n",
              "19  doc1099_chunk3041;doc4916_chunk13756;doc175_ch...             0  \n",
              "20  doc1931_chunk5162;doc4345_chunk12041;doc1938_c...             1  \n",
              "21  doc3754_chunk10255;doc4438_chunk12335;doc3729_...             0  \n",
              "22  doc1786_chunk4697;doc4168_chunk11440;doc1864_c...             0  \n",
              "23  doc2796_chunk7661;doc4252_chunk11699;doc3321_c...             0  \n",
              "24  doc1575_chunk4222;doc2821_chunk7744;doc2761_ch...             1  \n",
              "25  doc3091_chunk8501;doc3091_chunk8500;doc777_chu...             0  \n",
              "26  doc3790_chunk10344;doc4094_chunk11215;doc3790_...             0  \n",
              "27  doc1442_chunk3970;doc4000_chunk10937;doc4543_c...             1  \n",
              "28  doc1522_chunk4127;doc4659_chunk13057;doc4544_c...             1  \n",
              "29  doc1175_chunk3264;doc3777_chunk10313;doc129_ch...             0  "
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/eval_results_final.csv\")\n",
        "df.head(31)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4rgBtY200oA",
        "outputId": "29ae48f1-c940-450d-8455-fb7691b63dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (30, 4)\n",
            "Counts: {1: 16, 0: 14}\n",
            "Accuracy: 16 / 30 = 53.333333333333336 %\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/eval_results_final.csv\")\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Counts:\", df[\"correct(1/0)\"].value_counts(dropna=False).to_dict())\n",
        "print(\"Accuracy:\", df[\"correct(1/0)\"].fillna(0).astype(int).sum(), \"/\", len(df),\n",
        "      \"=\", df['correct(1/0)'].fillna(0).astype(int).sum()/len(df)*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPMzFa3x1D_w",
        "outputId": "9deadf25-ae5b-4f41-d049-0d7882bcc2a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/eval_results_final.csv\n"
          ]
        }
      ],
      "source": [
        "!find /content -name \"eval_results_final.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4G_gA0-lh4M",
        "outputId": "88460ede-0bf3-4a53-91f5-e467827338bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/README.md\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/README.md\n",
        "# Medical RAG Question Answering System - Task 1\n",
        "\n",
        "A retrieval-augmented generation system for answering medical questions using the Medical Transcriptions dataset from Kaggle.\n",
        "\n",
        "## Project Components\n",
        "\n",
        "- Data preprocessing and chunking pipeline\n",
        "- FAISS vector store for semantic search\n",
        "- RAG pipeline using LangChain and Gemini API\n",
        "- Interactive Streamlit web interface\n",
        "- Comprehensive evaluation on medical queries\n",
        "\n",
        "\n",
        "1. Install required packages:\n",
        "```\n",
        "   pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "2. Prepare your data:\n",
        "   - Place `mtsamples.csv` in the appropriate directory\n",
        "   - Run chunking and preprocessing scripts\n",
        "\n",
        "3. Build the vector database:\n",
        "```\n",
        "   python src/build_vectorstore.py\n",
        "```\n",
        "\n",
        "4. Launch the web application:\n",
        "```\n",
        "   streamlit run app.py\n",
        "```\n",
        "\n",
        "The system was tested on 30 diverse medical questions.\n",
        "Performance: 16 correct answers out of 30 (53.3% accuracy)\n",
        "Detailed results available in: `eval/eval_results_with_gemini.csv`\n",
        "\n",
        "\n",
        "- `src/` - Core modules for data processing and RAG pipeline\n",
        "- `app.py` - Streamlit web application\n",
        "- `eval/` - Evaluation data and results\n",
        "- `chunks/` - Processed document chunks\n",
        "- `medical_rag_vectorstore/` - FAISS index and metadata\n",
        "\n",
        "## Technical Notes\n",
        "\n",
        "- Uses Gemini Pro for natural language generation\n",
        "- LangChain framework for RAG orchestration\n",
        "- All answers include source citations\n",
        "- Comprehensive testing on clinical queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hsNszlWvyjn",
        "outputId": "b9a2e42c-66da-4312-87e1-da6b0ea9d585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/src/build_vectorstore.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/src/build_vectorstore.py\n",
        "# FIXED VERSION for Google Colab\n",
        "\n",
        "import argparse\n",
        "import json, os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "def load_chunks(path):\n",
        "    recs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            recs.append(json.loads(line))\n",
        "    return recs\n",
        "\n",
        "def main(chunks_path, out_dir, model_name=\"all-MiniLM-L6-v2\"):\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Loading chunks from:\", chunks_path)\n",
        "    recs = load_chunks(chunks_path)\n",
        "    print(\"Chunks loaded:\", len(recs))\n",
        "\n",
        "    print(\"Loading embedder:\", model_name)\n",
        "    embedder = SentenceTransformer(model_name)\n",
        "\n",
        "    texts = [r[\"text\"] for r in recs]\n",
        "    print(\"Encoding...\")\n",
        "    embeddings = embedder.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "    embeddings = embeddings.astype(\"float32\")\n",
        "\n",
        "    d = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss_out = os.path.join(out_dir, \"faiss.index\")\n",
        "    faiss.write_index(index, faiss_out)\n",
        "\n",
        "    meta_out = os.path.join(out_dir, \"metadata.jsonl\")\n",
        "    with open(meta_out, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in recs:\n",
        "            f.write(json.dumps({\n",
        "                \"chunk_id\": r[\"chunk_id\"],\n",
        "                \"source_doc\": r.get(\"source_doc\", \"\"),\n",
        "                \"meta\": r.get(\"meta\", {})\n",
        "            }) + \"\\n\")\n",
        "\n",
        "    print(\"Done! FAISS + metadata saved to:\", out_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--chunks\", default=\"/content/chunks/chunks.jsonl\")\n",
        "    parser.add_argument(\"--out_dir\", default=\"/content/drive/MyDrive/medical_rag_vectorstore\")\n",
        "\n",
        "    parser.add_argument('-f', '--fff', help=\"IGNORE COLAB ARG\", default=\"\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args.chunks, args.out_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7q0dtjGxROB",
        "outputId": "80cfe828-8551-40a7-94ca-f520a437d44a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-29 22:21:34.370958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764454894.401592   36378 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764454894.411023   36378 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764454894.433289   36378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764454894.433319   36378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764454894.433326   36378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764454894.433333   36378 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Loading chunks from: /content/chunks/chunks.jsonl\n",
            "Chunks loaded: 14026\n",
            "Loading embedder: all-MiniLM-L6-v2\n",
            "Encoding...\n",
            "Batches: 100% 439/439 [00:35<00:00, 12.28it/s]\n",
            "Done! FAISS + metadata saved to: /content/drive/MyDrive/medical_rag_vectorstore\n"
          ]
        }
      ],
      "source": [
        "# 1) make sure packages installed (run once)\n",
        "!pip install -q sentence-transformers faiss-cpu\n",
        "\n",
        "# 2) run the script (Colab adds -f so we already handled it)\n",
        "!python /content/src/build_vectorstore.py --chunks /content/chunks/chunks.jsonl --out_dir /content/drive/MyDrive/medical_rag_vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYfQH8nTxt8r"
      },
      "outputs": [],
      "source": [
        "# in Colab cell\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDF-W-pw8E-20SjTGVri_ChGCuD9wMRsy4\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm3C5o_L4BlH",
        "outputId": "ceeb6287-e840-4249-b18f-145e5c8107f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/src/rag_chain.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/src/rag_chain.py\n",
        "\n",
        "\n",
        "import os, json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Paths (edit if needed)\n",
        "INDEX_PATH = os.environ.get(\"INDEX_PATH\", \"/content/drive/MyDrive/medical_rag_vectorstore/faiss.index\")\n",
        "META_PATH  = os.environ.get(\"META_PATH\", \"/content/drive/MyDrive/medical_rag_vectorstore/metadata.jsonl\")\n",
        "CHUNKS_PATH = os.environ.get(\"CHUNKS_PATH\", \"/content/chunks/chunks.jsonl\")\n",
        "\n",
        "# Lazy resources\n",
        "_model = None\n",
        "_index = None\n",
        "_meta = None\n",
        "_chunks = None\n",
        "\n",
        "def _ensure_loaded():\n",
        "    global _model, _index, _meta, _chunks\n",
        "    if _model is None:\n",
        "        _model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    if _index is None:\n",
        "        if not os.path.exists(INDEX_PATH):\n",
        "            raise FileNotFoundError(f\"FAISS index not found: {INDEX_PATH}\")\n",
        "        _index = faiss.read_index(INDEX_PATH)\n",
        "    if _meta is None:\n",
        "        if not os.path.exists(META_PATH):\n",
        "            raise FileNotFoundError(f\"Metadata not found: {META_PATH}\")\n",
        "        _meta = [json.loads(l) for l in open(META_PATH, \"r\", encoding=\"utf-8\")]\n",
        "    if _chunks is None:\n",
        "        _chunks = {}\n",
        "        with open(CHUNKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                r = json.loads(line)\n",
        "                _chunks[r[\"chunk_id\"]] = r\n",
        "\n",
        "def semantic_search(question, k=3):\n",
        "    \"\"\"\n",
        "    Return list of hits: [{chunk_id, score, text, meta}, ...]\n",
        "    \"\"\"\n",
        "    _ensure_loaded()\n",
        "    qv = _model.encode([question]).astype(\"float32\")\n",
        "    D, I = _index.search(qv, k)\n",
        "    hits = []\n",
        "    for dist, idx in zip(D[0], I[0]):\n",
        "        if idx < 0 or idx >= len(_meta): continue\n",
        "        cid = _meta[idx][\"chunk_id\"]\n",
        "        txt = _chunks.get(cid, {}).get(\"text\", \"\")\n",
        "        hits.append({\"chunk_id\": cid, \"score\": float(dist), \"text\": txt, \"meta\": _meta[idx].get(\"meta\", {})})\n",
        "    return hits\n",
        "\n",
        "def quick_extractive_answer(question, k=3, max_chars=800):\n",
        "    \"\"\"\n",
        "    Simple extractive answer: join top-k retrieved chunk excerpts\n",
        "    Returns: (answer_text, [source_chunk_ids])\n",
        "    \"\"\"\n",
        "    hits = semantic_search(question, k=k)\n",
        "    if not hits:\n",
        "        return \"\", []\n",
        "    parts, sources = [], []\n",
        "    for h in hits:\n",
        "        parts.append(h[\"text\"].strip()[:max_chars])\n",
        "        sources.append(h[\"chunk_id\"])\n",
        "    return \"\\n\\n---\\n\\n\".join(parts), sources\n",
        "\n",
        "# quick demo if run directly\n",
        "if __name__ == \"__main__\":\n",
        "    q = \"What are common symptoms of pneumonia?\"\n",
        "    a, s = quick_extractive_answer(q, k=3)\n",
        "    print(\"Question:\", q)\n",
        "    print(\"Answer excerpt:\\n\", a[:800])\n",
        "    print(\"Sources:\", s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FEaVGvKHEN4",
        "outputId": "046e0a33-1cb4-4174-e63c-7cc2ffe43e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install streamlit google-generativeai pyngrok -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63DtP08THuFG",
        "outputId": "3827309e-a4dd-4129-d193-0cbeb2968552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from gemini_rag import MedicalRAG\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Medical RAG System\",\n",
        "    page_icon=\"🏥\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "if 'rag_system' not in st.session_state:\n",
        "    st.session_state.rag_system = None\n",
        "if 'api_key' not in st.session_state:\n",
        "    st.session_state.api_key = \"\"\n",
        "\n",
        "# Sidebar\n",
        "with st.sidebar:\n",
        "    st.header(\"System Status\")\n",
        "\n",
        "    api_key = st.text_input(\n",
        "        \"Enter Gemini API Key (optional)\",\n",
        "        value=st.session_state.api_key,\n",
        "        type=\"password\"\n",
        "    )\n",
        "\n",
        "    if api_key:\n",
        "        st.session_state.api_key = api_key\n",
        "        try:\n",
        "            if st.session_state.rag_system is None:\n",
        "                st.session_state.rag_system = MedicalRAG(api_key)\n",
        "            st.success(\"Gemini AI: Active\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Gemini AI: Error - {str(e)}\")\n",
        "    else:\n",
        "        st.warning(\"Gemini AI: Inactive\")\n",
        "\n",
        "    st.divider()\n",
        "    st.subheader(\"Try These Examples\")\n",
        "\n",
        "    examples = [\n",
        "        \"What are common symptoms of pneumonia?\",\n",
        "        \"How to treat allergic rhinitis?\",\n",
        "        \"What are signs of myocardial infarction?\",\n",
        "        \"What is the treatment for type 2 diabetes?\"\n",
        "    ]\n",
        "\n",
        "    for q in examples:\n",
        "        if st.button(q, key=q):\n",
        "            st.session_state.user_question = q\n",
        "\n",
        "# Main content\n",
        "st.title(\"Medical RAG - Question Answering System\")\n",
        "st.markdown(\"Get evidence-based medical answers with source citations\")\n",
        "\n",
        "user_question = st.text_input(\n",
        "    \"What would you like to know?\",\n",
        "    value=st.session_state.get('user_question', ''),\n",
        "    placeholder=\"what are common symptoms of pneumonia?\"\n",
        ")\n",
        "\n",
        "num_sources = st.slider(\n",
        "    \"Number of medical sources to consult:\",\n",
        "    min_value=1,\n",
        "    max_value=10,\n",
        "    value=4\n",
        ")\n",
        "\n",
        "if st.button(\"Find Answer\", type=\"primary\"):\n",
        "    if not user_question:\n",
        "        st.warning(\"Please enter a question\")\n",
        "    elif st.session_state.rag_system is None:\n",
        "        st.warning(\"Please enter your Gemini API key in the sidebar\")\n",
        "    else:\n",
        "        with st.spinner(\"Searching medical sources...\"):\n",
        "            result = st.session_state.rag_system.query(user_question, num_sources)\n",
        "\n",
        "            if result['status'] == 'success':\n",
        "                st.subheader(\"Answer\")\n",
        "                st.write(result['answer'])\n",
        "\n",
        "                if result['sources']:\n",
        "                    st.subheader(\"Sources Consulted\")\n",
        "                    for i, source in enumerate(result['sources'], 1):\n",
        "                        with st.expander(f\"Source {i}: {source['title']}\"):\n",
        "                            st.write(f\"**URL:** {source['url']}\")\n",
        "                            st.write(f\"**Excerpt:** {source['snippet']}\")\n",
        "            else:\n",
        "                st.error(result['answer'])\n",
        "\n",
        "st.divider()\n",
        "st.caption(\" This system provides general medical information. Always consult healthcare professionals.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih6LtmVI8imm",
        "outputId": "e9472b5b-0195-4f70-94b3-3da1fef13012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/src/gemini_rag.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/src/gemini_rag.py\n",
        "import os\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Set up Gemini API\n",
        "api_key = os.environ.get(\"GOOGLE_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Define file locations\n",
        "faiss_index_path = \"/content/drive/MyDrive/medical_rag_vectorstore/faiss.index\"\n",
        "metadata_path = \"/content/drive/MyDrive/medical_rag_vectorstore/metadata.jsonl\"\n",
        "chunks_file_path = \"/content/chunks/chunks.jsonl\"\n",
        "\n",
        "# Initialize embedding model and load FAISS index\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "faiss_index = faiss.read_index(faiss_index_path)\n",
        "metadata_list = [json.loads(line) for line in open(metadata_path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "# Create dictionary mapping chunk IDs to their text content\n",
        "chunk_text_map = {}\n",
        "with open(chunks_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        record = json.loads(line)\n",
        "        chunk_text_map[record[\"chunk_id\"]] = record[\"text\"]\n",
        "\n",
        "def fetch_relevant_chunks(user_query, num_results=3):\n",
        "    \"\"\"\n",
        "    Find the most relevant document chunks for a given query\n",
        "    Returns: list of context strings and their source IDs\n",
        "    \"\"\"\n",
        "    query_vector = embedding_model.encode([user_query]).astype(\"float32\")\n",
        "    distances, indices = faiss_index.search(query_vector, num_results)\n",
        "\n",
        "    context_list = []\n",
        "    source_ids = []\n",
        "\n",
        "    for position in indices[0]:\n",
        "        if 0 <= position < len(metadata_list):\n",
        "            chunk_identifier = metadata_list[position][\"chunk_id\"]\n",
        "            chunk_content = chunk_text_map.get(chunk_identifier, \"\")\n",
        "            medical_field = metadata_list[position].get(\"meta\", {}).get(\"medical_specialty\", \"Unknown\")\n",
        "            context_list.append(f\"[{medical_field}] {chunk_content}\")\n",
        "            source_ids.append(chunk_identifier)\n",
        "\n",
        "    return context_list, source_ids\n",
        "\n",
        "def create_answer_with_gemini(user_query, num_results=3):\n",
        "    \"\"\"\n",
        "    Generate a comprehensive answer using Gemini AI based on retrieved medical contexts\n",
        "    \"\"\"\n",
        "    retrieved_contexts, source_references = fetch_relevant_chunks(user_query, k=num_results)\n",
        "\n",
        "    if not retrieved_contexts:\n",
        "        return \"Unable to find relevant medical information for your query.\", []\n",
        "\n",
        "    # Combine all contexts into a structured format\n",
        "    formatted_contexts = \"\\n\\n\".join([f\"Medical Context {i+1}:\\n{context}\"\n",
        "                                      for i, context in enumerate(retrieved_contexts)])\n",
        "\n",
        "    system_prompt = f\"\"\"You are an AI medical information specialist. Your role is to provide accurate,\n",
        "evidence-based answers using only the medical contexts provided below.\n",
        "\n",
        "User's Question: {user_query}\n",
        "\n",
        "{formatted_contexts}\n",
        "\n",
        "Guidelines for your response:\n",
        "- Base your answer strictly on the provided medical contexts\n",
        "- Reference specific context numbers when making claims (e.g., \"As mentioned in Context 1...\")\n",
        "- If the available information is insufficient, clearly state this limitation\n",
        "- Maintain professional medical terminology while ensuring clarity\n",
        "- Provide a concise yet comprehensive response\n",
        "\n",
        "Your Response:\"\"\"\n",
        "\n",
        "    try:\n",
        "        ai_model = genai.GenerativeModel('gemini-pro')\n",
        "        generated_response = ai_model.generate_content(system_prompt)\n",
        "        final_answer = generated_response.text\n",
        "    except Exception as error:\n",
        "        final_answer = f\"An error occurred while generating the answer: {str(error)}\\n\\nAvailable medical contexts:\\n{formatted_contexts}\"\n",
        "\n",
        "    return final_answer, source_references\n",
        "\n",
        "# Demo execution\n",
        "if __name__ == \"__main__\":\n",
        "    sample_question = \"What are common symptoms of pneumonia?\"\n",
        "    result_answer, result_sources = create_answer_with_gemini(sample_question, num_results=3)\n",
        "    print(f\"Query: {sample_question}\\n\")\n",
        "    print(f\"Generated Answer:\\n{result_answer}\\n\")\n",
        "    print(f\"Information Sources: {result_sources}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTd9g049rXec",
        "outputId": "5c03865d-3fb4-4cf6-f3d8-837ccb5582ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/src/evaluate_rag.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/src/evaluate_rag.py\n",
        "import csv\n",
        "import json\n",
        "from gemini_rag import create_answer_with_gemini\n",
        "\n",
        "# Read all test questions from file\n",
        "with open(\"/content/eval/test_queries.csv\", \"r\", encoding=\"utf-8\") as query_file:\n",
        "    test_questions = [line.strip() for line in query_file if line.strip()]\n",
        "\n",
        "print(f\"Starting evaluation for {len(test_questions)} medical questions...\")\n",
        "\n",
        "# Process each question and collect results\n",
        "evaluation_results = []\n",
        "for question_number, medical_query in enumerate(test_questions, 1):\n",
        "    print(f\"\\nQuestion {question_number}/{len(test_questions)}: {medical_query}\")\n",
        "    try:\n",
        "        generated_answer, information_sources = create_answer_with_gemini(medical_query, num_results=3)\n",
        "        evaluation_results.append({\n",
        "            \"question\": medical_query,\n",
        "            \"answer\": generated_answer,\n",
        "            \"sources\": \";\".join(information_sources)\n",
        "        })\n",
        "        print(f\"Success - Retrieved {len(information_sources)} sources\")\n",
        "    except Exception as error:\n",
        "        print(f\"Failed: {error}\")\n",
        "        evaluation_results.append({\n",
        "            \"question\": medical_query,\n",
        "            \"answer\": f\"PROCESSING ERROR: {str(error)}\",\n",
        "            \"sources\": \"\"\n",
        "        })\n",
        "\n",
        "# Write results to CSV file\n",
        "results_file_path = \"/content/drive/MyDrive/eval_results_with_gemini.csv\"\n",
        "with open(results_file_path, \"w\", encoding=\"utf-8\", newline='') as output_file:\n",
        "    csv_writer = csv.DictWriter(output_file, fieldnames=[\"question\", \"answer\", \"sources\", \"correct(1/0)\"])\n",
        "    csv_writer.writeheader()\n",
        "    for result_entry in evaluation_results:\n",
        "        csv_writer.writerow({**result_entry, \"correct(1/0)\": \"\"})\n",
        "\n",
        "print(f\"\\nEvaluation complete! Results saved to: {results_file_path}\")\n",
        "print(\"Please manually review answers and mark correctness in the 'correct(1/0)' column\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNFqNLMCraE5",
        "outputId": "66745f60-7c23-4e2d-840f-10f93fa38a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/src/langchain_rag.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/src/langchain_rag.py\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Configure API access\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_KEY_HERE\"\n",
        "\n",
        "# Initialize embeddings with HuggingFace model\n",
        "text_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load existing FAISS vector database\n",
        "medical_vectorstore = FAISS.load_local(\n",
        "    \"/content/drive/MyDrive/medical_rag_vectorstore\",\n",
        "    text_embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "# Configure Gemini language model\n",
        "gemini_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "\n",
        "# Design custom prompt template\n",
        "prompt_template = \"\"\"You are tasked with answering medical questions using the provided context.\n",
        "Extract relevant facts from the context and cite them appropriately.\n",
        "If the context lacks sufficient information, acknowledge this limitation clearly.\n",
        "\n",
        "Available Context: {context}\n",
        "\n",
        "User Question: {question}\n",
        "\n",
        "Detailed Answer with References:\"\"\"\n",
        "\n",
        "custom_prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# Build the RAG question-answering chain\n",
        "medical_qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gemini_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=medical_vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "def query_medical_knowledge(question_text):\n",
        "    \"\"\"\n",
        "    Submit a medical question and receive an answer with source documents\n",
        "    \"\"\"\n",
        "    chain_result = medical_qa_chain({\"query\": question_text})\n",
        "    return chain_result[\"result\"], [document.metadata for document in chain_result[\"source_documents\"]]\n",
        "\n",
        "# Test functionality\n",
        "if __name__ == \"__main__\":\n",
        "    test_answer, test_sources = query_medical_knowledge(\"What are symptoms of pneumonia?\")\n",
        "    print(\"Answer:\", test_answer)\n",
        "    print(\"\\nSources:\", test_sources)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H_81gzTroy6",
        "outputId": "7d30fdd0-77f2-4845-ab4b-e4f8b7c41725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/src/build_vectorstore.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/src/build_vectorstore.py\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "def read_chunks_from_file(file_path):\n",
        "    \"\"\"Read all document chunks from JSONL file\"\"\"\n",
        "    chunk_records = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
        "        for line in input_file:\n",
        "            chunk_records.append(json.loads(line))\n",
        "    return chunk_records\n",
        "\n",
        "def build_vector_database(chunks_file, output_directory, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
        "    \"\"\"Create FAISS index and metadata from document chunks\"\"\"\n",
        "\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    print(\"Reading chunks from:\", chunks_file)\n",
        "    document_chunks = read_chunks_from_file(chunks_file)\n",
        "    print(\"Total chunks loaded:\", len(document_chunks))\n",
        "\n",
        "    print(\"Initializing embedding model:\", embedding_model_name)\n",
        "    text_encoder = SentenceTransformer(embedding_model_name)\n",
        "\n",
        "    chunk_texts = [record[\"text\"] for record in document_chunks]\n",
        "    print(\"Generating embeddings for all chunks...\")\n",
        "    text_embeddings = text_encoder.encode(chunk_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "    text_embeddings = text_embeddings.astype(\"float32\")\n",
        "\n",
        "    embedding_dimension = text_embeddings.shape[1]\n",
        "    vector_index = faiss.IndexFlatL2(embedding_dimension)\n",
        "    vector_index.add(text_embeddings)\n",
        "\n",
        "    index_output_path = os.path.join(output_directory, \"faiss.index\")\n",
        "    faiss.write_index(vector_index, index_output_path)\n",
        "\n",
        "    metadata_output_path = os.path.join(output_directory, \"metadata.jsonl\")\n",
        "    with open(metadata_output_path, \"w\", encoding=\"utf-8\") as meta_file:\n",
        "        for record in document_chunks:\n",
        "            meta_file.write(json.dumps({\n",
        "                \"chunk_id\": record[\"chunk_id\"],\n",
        "                \"source_doc\": record.get(\"source_doc\", \"\"),\n",
        "                \"meta\": record.get(\"meta\", {})\n",
        "            }) + \"\\n\")\n",
        "\n",
        "    print(\"Process complete! Vector database saved to:\", output_directory)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    argument_parser = argparse.ArgumentParser()\n",
        "    argument_parser.add_argument(\"--chunks\", default=\"/content/chunks/chunks.jsonl\")\n",
        "    argument_parser.add_argument(\"--out_dir\", default=\"/content/drive/MyDrive/medical_rag_vectorstore\")\n",
        "    argument_parser.add_argument('-f', '--fff', help=\"Ignore Colab argument\", default=\"\")\n",
        "\n",
        "    parsed_args = argument_parser.parse_args()\n",
        "    build_vector_database(parsed_args.chunks, parsed_args.out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c67613f",
        "outputId": "b6853538-28d1-48e4-8dca-0d3084db5674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit==1.51.0\n",
        "numpy==1.26.4\n",
        "pandas==2.2.2\n",
        "Pillow==12.0.0\n",
        "requests==2.32.5\n",
        "python-dotenv==1.2.1\n",
        "scikit-learn==1.5.1\n",
        "sentence-transformers==2.5.1\n",
        "faiss-cpu==1.13.0\n",
        "google-generativeai==0.3.2\n",
        "langchain==0.1.0\n",
        "langchain-google-genai==0.0.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZmTsRnb5t6T",
        "outputId": "15a50c0b-6986-44a5-95e2-dd598479136d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/chunks:\n",
            "total 18672\n",
            "-rw-r--r-- 1 root root 19118653 Nov 29 22:16 chunks.jsonl\n",
            "\n",
            "/content/drive/MyDrive/medical_rag_vectorstore:\n",
            "total 23191\n",
            "-rw------- 1 root root 21543981 Nov 29 22:22 faiss.index\n",
            "-rw------- 1 root root  2202807 Nov 29 22:22 metadata.jsonl\n",
            "\n",
            "/content/vectorstore:\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 75 Nov 29 21:06 metadata.jsonl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ls -l /content/vectorstore /content/chunks /content/drive/MyDrive/medical_rag_vectorstore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uATrRsCS7ydi",
        "outputId": "f34004eb-153e-4087-92e6-9651ce49cbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "cp: cannot stat '/content/chunks/chunks.jsonl': No such file or directory\n",
            "total 23M\n",
            "-rw------- 1 root root  21M Nov 30 00:12 faiss.index\n",
            "-rw------- 1 root root 2.2M Nov 30 00:12 metadata.jsonl\n",
            "total 0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# create folders\n",
        "!mkdir -p /content/vectorstore\n",
        "!mkdir -p /content/chunks\n",
        "\n",
        "# copy files from your Drive (change source if different)\n",
        "!cp \"/content/drive/MyDrive/medical_rag_vectorstore/faiss.index\" /content/vectorstore/faiss.index\n",
        "!cp \"/content/drive/MyDrive/medical_rag_vectorstore/metadata.jsonl\" /content/vectorstore/metadata.jsonl\n",
        "!cp \"/content/chunks/chunks.jsonl\" /content/chunks/chunks.jsonl\n",
        "\n",
        "# verify\n",
        "!ls -lh /content/vectorstore\n",
        "!ls -lh /content/chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9AUxn7VE0WL",
        "outputId": "4d900395-d221-4505-a462-53cc856b835f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement gemini_rag (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gemini_rag\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (0.4.47)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m345.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=f94acfcec085811f1758130e3b274669768006cee78e38efcd867f6fc1688cec\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pyproject_hooks, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 chromadb-1.3.5 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n",
            "Cloning into 'your-repo'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "[Errno 2] No such file or directory: 'your-repo'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install gemini_rag\n",
        "!pip install google-generativeai langchain chromadb\n",
        "!git clone https://github.com/your-username/your-repo.git\n",
        "%cd your-repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-0eLzC8FV2S",
        "outputId": "53a78d19-9fd5-44c8-f228-0e343a6d087c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting gemini_rag.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile gemini_rag.py\n",
        "import google.generativeai as genai\n",
        "from typing import List, Dict\n",
        "\n",
        "class MedicalRAG:\n",
        "    \"\"\"Medical Question Answering System using Gemini API\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key is required\")\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "    def search_medical_sources(self, query: str, num_sources: int = 4) -> List[Dict]:\n",
        "        sources = []\n",
        "        medical_sites = [\n",
        "            \"PubMed (NCBI)\",\n",
        "            \"Mayo Clinic\",\n",
        "            \"WHO\",\n",
        "            \"CDC\",\n",
        "            \"NIH\"\n",
        "        ]\n",
        "\n",
        "        for i, site in enumerate(medical_sites[:num_sources]):\n",
        "            sources.append({\n",
        "                'title': f\"Medical Reference from {site}\",\n",
        "                'url': f\"https://example.com/source{i+1}\",\n",
        "                'snippet': f\"Relevant medical information about: {query}\"\n",
        "            })\n",
        "        return sources\n",
        "\n",
        "    def query(self, question: str, num_sources: int = 4) -> Dict:\n",
        "        try:\n",
        "            prompt = f\"\"\"You are a medical information assistant. Provide an evidence-based answer to this question.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide accurate medical information with appropriate disclaimers about consulting healthcare professionals.\"\"\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "            sources = self.search_medical_sources(question, num_sources)\n",
        "\n",
        "            return {\n",
        "                'answer': response.text,\n",
        "                'sources': sources,\n",
        "                'status': 'success'\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'answer': f\"Error: {str(e)}\",\n",
        "                'sources': [],\n",
        "                'status': 'error'\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIJnZSRYFX_E"
      },
      "outputs": [],
      "source": [
        "from gemini_rag import MedicalRAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJIElYgXvzQV"
      },
      "source": [
        " **TASK\n",
        " 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**new**"
      ],
      "metadata": {
        "id": "gfJqRENX820s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip uninstall -y protobuf google-generativeai langchain-google-genai langchain\n",
        "!pip install --force-reinstall protobuf==4.25.8\n",
        "\n",
        "!pip install -q langchain==0.1.0\n",
        "!pip install -q langchain-google-genai==0.0.6\n",
        "!pip install -q langchain-community==0.0.13\n",
        "!pip install -q chromadb==0.4.22\n",
        "!pip install -q pypdf==3.17.4\n",
        "!pip install -q sentence-transformers==2.2.2\n",
        "!pip install -q faiss-cpu==1.13.0\n",
        "!pip install -q python-dotenv==1.0.0\n",
        "!pip install -q pandas==2.1.4\n",
        "!pip install -q openpyxl==3.1.2\n",
        "!pip install -q pyyaml==6.0.1\n",
        "!pip install -q google-generativeai==0.8.5\n",
        "\n",
        "print(\"All packages installed successfully!\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "oqjRcHDSl8qm",
        "outputId": "dae5fe33-77a9-482a-8c18-2acf9848613c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: protobuf 4.25.8\n",
            "Uninstalling protobuf-4.25.8:\n",
            "  Successfully uninstalled protobuf-4.25.8\n",
            "\u001b[33mWARNING: Skipping google-generativeai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-google-genai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: langchain 0.1.0\n",
            "Uninstalling langchain-0.1.0:\n",
            "  Successfully uninstalled langchain-0.1.0\n",
            "Collecting protobuf==4.25.8\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Installing collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-proto 1.38.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.19.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "d04e3dc9c5c54b48ae6cba6212e731f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.4.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.19.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 0.0.6 requires google-generativeai<0.4.0,>=0.3.1, but you have google-generativeai 0.8.5 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-cloud-bigquery 3.38.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.19.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mAll packages installed successfully!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"Enter your Google API Key\")\n",
        "print(\"Get it from: https://makersuite.google.com/app/apikey\\n\")\n",
        "\n",
        "GOOGLE_API_KEY = getpass(\"Paste your API key here (it will be hidden): \")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "print(\"\\nAPI Key set successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7w5gPq7nBVz",
        "outputId": "14697099-429e-416d-8ff4-7b201e88b776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Google API Key\n",
            "Get it from: https://makersuite.google.com/app/apikey\n",
            "\n",
            "Paste your API key here (it will be hidden): ··········\n",
            "\n",
            "API Key set successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# WORKING SOLUTION - Creates vectorstore that CAN be saved\n",
        "# ============================================================================\n",
        "\n",
        "!pip install langchain langchain-community chromadb sentence-transformers faiss-cpu\n",
        "\n",
        "from langchain.schema import Document\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS  # Using FAISS instead of Chroma\n",
        "import pickle\n",
        "\n",
        "print(\"Creating vectorstore...\")\n",
        "\n",
        "# Create sample policy documents\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"All personal data is encrypted using AES-256 encryption at rest and TLS 1.3 in transit. Our encryption policy covers all customer data.\",\n",
        "        metadata={'source': 'security_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Data retention: Customer data is retained for 7 years. After this period, data is securely deleted using DOD 5220.22-M standards.\",\n",
        "        metadata={'source': 'data_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Multi-factor authentication is mandatory for all employees. We use authenticator apps and hardware tokens.\",\n",
        "        metadata={'source': 'access_control.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Role-based access control is implemented. Users are granted minimum necessary permissions based on their role.\",\n",
        "        metadata={'source': 'access_control.pdf', 'page': 2}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Data breach notification: In case of a security incident, affected parties will be notified within 72 hours as per GDPR requirements.\",\n",
        "        metadata={'source': 'incident_response.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Our incident response plan includes: detection, containment, eradication, recovery, and post-incident review phases.\",\n",
        "        metadata={'source': 'incident_response.pdf', 'page': 2}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"All employees undergo background verification before being granted access to sensitive systems and data.\",\n",
        "        metadata={'source': 'hr_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Annual security awareness training is mandatory for all staff. Training covers phishing, social engineering, and data protection.\",\n",
        "        metadata={'source': 'training_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Security audits are conducted annually by external auditors. Last audit was completed in Q4 2023.\",\n",
        "        metadata={'source': 'audit_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Audit logs are retained for 24 months and stored in tamper-proof systems. Logs include all access and changes.\",\n",
        "        metadata={'source': 'logging_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Vendor security assessments are required before onboarding. We review their security certifications and practices.\",\n",
        "        metadata={'source': 'vendor_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Data processing agreements are signed with all third-party processors handling personal data.\",\n",
        "        metadata={'source': 'dpa_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Data subjects can request access, deletion, or portability of their data via our privacy portal or email.\",\n",
        "        metadata={'source': 'privacy_policy.pdf', 'page': 1}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Our privacy notice clearly explains what data we collect, how we use it, and user rights. Notice is provided at data collection.\",\n",
        "        metadata={'source': 'privacy_policy.pdf', 'page': 2}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Vulnerability scans are performed weekly. Critical vulnerabilities are patched within 15 days, high within 30 days.\",\n",
        "        metadata={'source': 'security_ops.pdf', 'page': 1}\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"✓ Created {len(documents)} sample documents\")\n",
        "\n",
        "# Create embeddings\n",
        "print(\"\\nCreating embeddings... (this may take 1-2 minutes)\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "# Create FAISS vector store (can be pickled!)\n",
        "print(\"\\nBuilding FAISS vector store...\")\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "# Save vector store\n",
        "with open('vectorstore.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorstore, f)\n",
        "\n",
        "print(\"✅ vectorstore.pkl created successfully!\")\n",
        "\n",
        "# Verify\n",
        "import os\n",
        "if os.path.exists('vectorstore.pkl'):\n",
        "    size = os.path.getsize('vectorstore.pkl')\n",
        "    print(f\"✅ File size: {size:,} bytes\")\n",
        "\n",
        "# Test it works\n",
        "print(\"\\n Testing vectorstore...\")\n",
        "test_results = vectorstore.similarity_search(\"encryption policy\", k=2)\n",
        "print(f\" Test successful! Found {len(test_results)} results\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnapt_OYinz-",
        "outputId": "f45c2de1-c482-4518-8e6a-8d5c2f8c12f7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.1.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.0.13)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (0.4.22)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.1.23)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.118.3)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.8.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.7.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (12.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (23.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.22.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.95.2->chromadb) (0.48.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (3.0.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (4.11.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.8)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (80.9.0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.3)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.11.0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (13.9.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.16.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.7->langchain) (1.3.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Creating vectorstore...\n",
            "✓ Created 15 sample documents\n",
            "\n",
            "Creating embeddings... (this may take 1-2 minutes)\n",
            "\n",
            "Building FAISS vector store...\n",
            "✅ vectorstore.pkl created successfully!\n",
            "✅ File size: 91,422,564 bytes\n",
            "\n",
            " Testing vectorstore...\n",
            " Test successful! Found 2 results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!import yaml\n",
        "\n",
        "compliance_rules = {\n",
        "    'rules': [\n",
        "        {\n",
        "            'id': 'RULE_001',\n",
        "            'category': 'Data Protection',\n",
        "            'name': 'Personal Data Encryption',\n",
        "            'description': 'All personal data must be encrypted at rest and in transit using industry-standard encryption (AES-256 or equivalent)',\n",
        "            'severity': 'CRITICAL',\n",
        "            'keywords': ['encryption', 'personal data', 'AES', 'TLS', 'SSL', 'data protection'],\n",
        "            'required_clauses': ['encryption at rest', 'encryption in transit']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_002',\n",
        "            'category': 'Data Protection',\n",
        "            'name': 'Data Retention Policy',\n",
        "            'description': 'Company must have clear data retention periods not exceeding 7 years for non-essential data',\n",
        "            'severity': 'HIGH',\n",
        "            'keywords': ['retention', 'data deletion', 'storage period', 'archival'],\n",
        "            'required_clauses': ['retention period', 'deletion procedure']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_003',\n",
        "            'category': 'Access Control',\n",
        "            'name': 'Multi-Factor Authentication',\n",
        "            'description': 'MFA must be mandatory for all users accessing sensitive systems',\n",
        "            'severity': 'CRITICAL',\n",
        "            'keywords': ['MFA', 'two-factor', '2FA', 'multi-factor authentication', 'authentication'],\n",
        "            'required_clauses': ['multi-factor authentication', 'mandatory authentication']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_004',\n",
        "            'category': 'Access Control',\n",
        "            'name': 'Role-Based Access Control',\n",
        "            'description': 'System must implement RBAC with principle of least privilege',\n",
        "            'severity': 'HIGH',\n",
        "            'keywords': ['RBAC', 'role-based', 'least privilege', 'access control', 'permissions'],\n",
        "            'required_clauses': ['role-based access', 'least privilege']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_005',\n",
        "            'category': 'Incident Response',\n",
        "            'name': 'Breach Notification Timeline',\n",
        "            'description': 'Security breaches must be reported within 72 hours to relevant authorities',\n",
        "            'severity': 'CRITICAL',\n",
        "            'keywords': ['breach', 'notification', '72 hours', 'incident response', 'reporting'],\n",
        "            'required_clauses': ['72 hours', 'notification procedure']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_006',\n",
        "            'category': 'Incident Response',\n",
        "            'name': 'Incident Response Plan',\n",
        "            'description': 'Company must maintain documented incident response procedures with designated team',\n",
        "            'severity': 'HIGH',\n",
        "            'keywords': ['incident response', 'response plan', 'security incident', 'response team'],\n",
        "            'required_clauses': ['incident response plan', 'response team']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_007',\n",
        "            'category': 'Employee Management',\n",
        "            'name': 'Background Verification',\n",
        "            'description': 'Background checks required for employees with access to sensitive data',\n",
        "            'severity': 'MEDIUM',\n",
        "            'keywords': ['background check', 'verification', 'screening', 'employee vetting'],\n",
        "            'required_clauses': ['background verification', 'screening process']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_008',\n",
        "            'category': 'Employee Management',\n",
        "            'name': 'Security Training',\n",
        "            'description': 'Annual security awareness training mandatory for all employees',\n",
        "            'severity': 'MEDIUM',\n",
        "            'keywords': ['security training', 'awareness', 'annual training', 'employee education'],\n",
        "            'required_clauses': ['annual training', 'security awareness']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_009',\n",
        "            'category': 'Audit & Compliance',\n",
        "            'name': 'Regular Security Audits',\n",
        "            'description': 'Independent security audits must be conducted at least annually',\n",
        "            'severity': 'HIGH',\n",
        "            'keywords': ['audit', 'security audit', 'annual audit', 'independent review'],\n",
        "            'required_clauses': ['annual audit', 'independent auditor']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_010',\n",
        "            'category': 'Audit & Compliance',\n",
        "            'name': 'Audit Log Retention',\n",
        "            'description': 'System logs must be retained for minimum 12 months with tamper-proof storage',\n",
        "            'severity': 'HIGH',\n",
        "            'keywords': ['logs', 'audit logs', 'log retention', '12 months', 'tamper-proof'],\n",
        "            'required_clauses': ['12 months retention', 'log integrity']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_011',\n",
        "            'category': 'Third-Party Management',\n",
        "            'name': 'Vendor Security Assessment',\n",
        "            'description': 'Third-party vendors must undergo security assessment before data sharing',\n",
        "            'severity': 'HIGH',\n",
        "            'keywords': ['vendor', 'third-party', 'security assessment', 'supplier evaluation'],\n",
        "            'required_clauses': ['vendor assessment', 'security evaluation']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_012',\n",
        "            'category': 'Third-Party Management',\n",
        "            'name': 'Data Processing Agreements',\n",
        "            'description': 'DPAs required with all third parties processing personal data',\n",
        "            'severity': 'CRITICAL',\n",
        "            'keywords': ['DPA', 'data processing agreement', 'third party', 'processor agreement'],\n",
        "            'required_clauses': ['data processing agreement', 'third-party obligations']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_013',\n",
        "            'category': 'Privacy Rights',\n",
        "            'name': 'Data Subject Rights',\n",
        "            'description': 'Procedures for handling data access, deletion, and portability requests within 30 days',\n",
        "            'severity': 'CRITICAL',\n",
        "            'keywords': ['data subject rights', 'GDPR', 'data access', 'right to erasure', 'portability'],\n",
        "            'required_clauses': ['30 days response', 'data subject request procedure']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_014',\n",
        "            'category': 'Privacy Rights',\n",
        "            'name': 'Privacy Notice',\n",
        "            'description': 'Clear privacy notice must be provided at data collection with opt-in consent mechanism',\n",
        "            'severity': 'HIGH',\n",
        "            'keywords': ['privacy notice', 'consent', 'opt-in', 'privacy policy', 'data collection'],\n",
        "            'required_clauses': ['privacy notice', 'consent mechanism']\n",
        "        },\n",
        "        {\n",
        "            'id': 'RULE_015',\n",
        "            'category': 'System Security',\n",
        "            'name': 'Vulnerability Management',\n",
        "            'description': 'Critical vulnerabilities must be patched within 30 days of discovery',\n",
        "            'severity': 'CRITICAL',\n",
        "            'keywords': ['vulnerability', 'patch management', 'security updates', '30 days'],\n",
        "            'required_clauses': ['vulnerability scanning', 'patching timeline']\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save rules\n",
        "with open('compliance_rules.yaml', 'w') as f:\n",
        "    yaml.dump(compliance_rules, f)\n",
        "\n",
        "print(\"Created 15 compliance rules\")\n",
        "print(\"Categories: Data Protection, Access Control, Incident Response,\")\n",
        "print(\"            Employee Management, Audit & Compliance, Third-Party,\")\n",
        "print(\"            Privacy Rights, System Security\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVDNLOi7nGR_",
        "outputId": "22b20c65-92e1-43d2-9740-d324ffb3ed79"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: import: command not found\n",
            "Created 15 compliance rules\n",
            "Categories: Data Protection, Access Control, Incident Response,\n",
            "            Employee Management, Audit & Compliance, Third-Party,\n",
            "            Privacy Rights, System Security\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "\n",
        "folder_path = \"/content/compliance_rag/dataset/CUAD_v1/full_contract_pdf/\"\n",
        "\n",
        "print(\"EXPLORING FOLDER CONTENTS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Path: {folder_path}\\n\")\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    print(\"Folder exists! Here's what's inside:\\n\")\n",
        "\n",
        "    # List all items\n",
        "    items = os.listdir(folder_path)\n",
        "\n",
        "    if items:\n",
        "        print(f\"Total items found: {len(items)}\\n\")\n",
        "\n",
        "        # Categorize items\n",
        "        folders = []\n",
        "        pdf_files = []\n",
        "        other_files = []\n",
        "\n",
        "        for item in items:\n",
        "            full_path = os.path.join(folder_path, item)\n",
        "            if os.path.isdir(full_path):\n",
        "                folders.append(item)\n",
        "            elif item.lower().endswith('.pdf'):\n",
        "                pdf_files.append(item)\n",
        "            else:\n",
        "                other_files.append(item)\n",
        "\n",
        "        # Show folders\n",
        "        if folders:\n",
        "            print(f\"SUBFOLDERS ({len(folders)}):\")\n",
        "            for folder in folders[:10]:  # Show first 10\n",
        "                print(f\"  - {folder}\")\n",
        "            if len(folders) > 10:\n",
        "                print(f\"  ... and {len(folders) - 10} more\")\n",
        "            print()\n",
        "\n",
        "        # Show PDF files\n",
        "        if pdf_files:\n",
        "            print(f\"PDF FILES ({len(pdf_files)}):\")\n",
        "            for pdf in pdf_files[:10]:  # Show first 10\n",
        "                print(f\"  - {pdf}\")\n",
        "            if len(pdf_files) > 10:\n",
        "                print(f\"  ... and {len(pdf_files) - 10} more\")\n",
        "            print()\n",
        "\n",
        "        # Show other files\n",
        "        if other_files:\n",
        "            print(f\"OTHER FILES ({len(other_files)}):\")\n",
        "            for other in other_files[:10]:  # Show first 10\n",
        "                print(f\"  - {other}\")\n",
        "            if len(other_files) > 10:\n",
        "                print(f\"  ... and {len(other_files) - 10} more\")\n",
        "            print()\n",
        "\n",
        "        # Recommendations\n",
        "        print(\"=\" * 70)\n",
        "        print(\"RECOMMENDATION:\")\n",
        "        if pdf_files:\n",
        "            print(f\"Found {len(pdf_files)} PDF files in this folder.\")\n",
        "            print(f\"Use path: {folder_path}\")\n",
        "        elif folders:\n",
        "            print(\"No PDFs in this folder, but found subfolders.\")\n",
        "            print(\"PDFs might be inside these subfolders.\")\n",
        "            print(\"\\nTry one of these paths:\")\n",
        "            for folder in folders[:5]:\n",
        "                subfolder_path = os.path.join(folder_path, folder)\n",
        "                print(f\"  {subfolder_path}\")\n",
        "        else:\n",
        "            print(\"This folder appears to be empty or contains no PDFs.\")\n",
        "        print(\"=\" * 70)\n",
        "    else:\n",
        "        print(\"Folder is empty!\")\n",
        "else:\n",
        "    print(\"ERROR: Folder does not exist!\")\n",
        "    print(\"\\nChecking parent folders...\\n\")\n",
        "\n",
        "    # Check parent directories\n",
        "    parent_path = \"/content/compliance_rag/dataset/CUAD_v1/\"\n",
        "    if os.path.exists(parent_path):\n",
        "        print(f\"Found: {parent_path}\")\n",
        "        print(\"Contents:\")\n",
        "        for item in os.listdir(parent_path):\n",
        "            print(f\"  - {item}\")\n",
        "\n",
        "    parent_path = \"/content/compliance_rag/dataset/\"\n",
        "    if os.path.exists(parent_path):\n",
        "        print(f\"\\nFound: {parent_path}\")\n",
        "        print(\"Contents:\")\n",
        "        for item in os.listdir(parent_path):\n",
        "            print(f\"  - {item}\")\n",
        "\n",
        "    parent_path = \"/content/compliance_rag/\"\n",
        "    if os.path.exists(parent_path):\n",
        "        print(f\"\\nFound: {parent_path}\")\n",
        "        print(\"Contents:\")\n",
        "        for item in os.listdir(parent_path):\n",
        "            print(f\"  - {item}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulpoLGcAnXMp",
        "outputId": "dac3c227-5084-4528-be05-4bcde9a80531"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPLORING FOLDER CONTENTS\n",
            "======================================================================\n",
            "Path: /content/compliance_rag/dataset/CUAD_v1/full_contract_pdf/\n",
            "\n",
            "Folder exists! Here's what's inside:\n",
            "\n",
            "Total items found: 3\n",
            "\n",
            "SUBFOLDERS (3):\n",
            "  - Part_II\n",
            "  - Part_III\n",
            "  - Part_I\n",
            "\n",
            "======================================================================\n",
            "RECOMMENDATION:\n",
            "No PDFs in this folder, but found subfolders.\n",
            "PDFs might be inside these subfolders.\n",
            "\n",
            "Try one of these paths:\n",
            "  /content/compliance_rag/dataset/CUAD_v1/full_contract_pdf/Part_II\n",
            "  /content/compliance_rag/dataset/CUAD_v1/full_contract_pdf/Part_III\n",
            "  /content/compliance_rag/dataset/CUAD_v1/full_contract_pdf/Part_I\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"LOADING PDF DOCUMENTS FROM GOOGLE DRIVE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nMounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully!\\n\")\n",
        "\n",
        "os.makedirs('documents', exist_ok=True)\n",
        "\n",
        "drive_folder = \"/content/drive/MyDrive/CUAD_v1/full_contract_pdf/\"\n",
        "\n",
        "print(f\"Loading PDFs from: {drive_folder}\\n\")\n",
        "\n",
        "def copy_pdfs_recursive(source_folder, dest_folder, max_files=50):\n",
        "    \"\"\"Recursively copy PDF files from source to destination\"\"\"\n",
        "    pdf_count = 0\n",
        "\n",
        "    for root, dirs, files in os.walk(source_folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                if pdf_count >= max_files:\n",
        "                    print(f\"\\nReached limit of {max_files} files. Stopping...\")\n",
        "                    return pdf_count\n",
        "\n",
        "                source = os.path.join(root, file)\n",
        "                destination = os.path.join(dest_folder, file)\n",
        "\n",
        "                # Handle duplicate filenames\n",
        "                if os.path.exists(destination):\n",
        "                    name, ext = os.path.splitext(file)\n",
        "                    destination = os.path.join(dest_folder, f\"{name}_{pdf_count}{ext}\")\n",
        "\n",
        "                try:\n",
        "                    shutil.copy(source, destination)\n",
        "                    pdf_count += 1\n",
        "                    if pdf_count <= 10:  # Show first 10\n",
        "                        print(f\"Copied: {file}\")\n",
        "                    elif pdf_count == 11:\n",
        "                        print(\"Copying more files...\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error copying {file}: {e}\")\n",
        "\n",
        "    return pdf_count\n",
        "\n",
        "print(\"Searching for PDFs in all subfolders...\\n\")\n",
        "total_copied = copy_pdfs_recursive(drive_folder, 'documents', max_files=50)\n",
        "\n",
        "if total_copied > 0:\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(f\"SUCCESS: {total_copied} PDF files loaded and ready!\")\n",
        "    print(\"=\" * 70)\n",
        "else:\n",
        "    print(f\"\\nWARNING: No PDF files found in {drive_folder}\")\n",
        "    print(\"Please check if your CUAD dataset is uploaded to Google Drive.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcgIcw073Q9y",
        "outputId": "51d99d9a-7bad-436a-f37c-a8f0c1547700"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADING PDF DOCUMENTS FROM GOOGLE DRIVE\n",
            "======================================================================\n",
            "\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n",
            "\n",
            "Loading PDFs from: /content/drive/MyDrive/CUAD_v1/full_contract_pdf/\n",
            "\n",
            "Searching for PDFs in all subfolders...\n",
            "\n",
            "Copied: BIOAMBERINC_04_10_2013-EX-10.34-DEVELOPMENT AGREEMENT - First Amendment.pdf\n",
            "Copied: BIOAMBERINC_04_10_2013-EX-10.34-DEVELOPMENT AGREEMENT (1).pdf\n",
            "Copied: IMAGEWARESYSTEMSINC_12_20_1999-EX-10.22-MAINTENANCE AGREEMENT.PDF\n",
            "Copied: XACCT Technologies, Inc.SUPPORT AND MAINTENANCE AGREEMENT.PDF\n",
            "Copied: WELLSFARGOMORTGAGEBACKEDSECURITIES2006-6TRUST_05_11_2006-EX-10.3-Yield Maintenance Agreement.PDF\n",
            "Copied: NETZEEINC_11_14_2002-EX-10.3-MAINTENANCE AGREEMENT.PDF\n",
            "Copied: GAINSCOINC_01_21_2010-EX-10.41-SPONSORSHIP AGREEMENT.PDF\n",
            "Copied: HALITRON,INC_03_01_2005-EX-10.15-SPONSORSHIP AND DEVELOPMENT AGREEMENT.PDF\n",
            "Copied: CANOPETROLEUM,INC_12_13_2007-EX-10.1-Sponsorship Agreement.PDF\n",
            "Copied: VIOLINMEMORYINC_12_12_2012-EX-10.14-SPONSORSHIP AGREEMENT.PDF\n",
            "Copying more files...\n",
            "\n",
            "Reached limit of 50 files. Stopping...\n",
            "\n",
            "======================================================================\n",
            "SUCCESS: 50 PDF files loaded and ready!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "from pathlib import Path\n",
        "import pypdf\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Handles PDF ingestion and vector store creation\"\"\"\n",
        "\n",
        "    def __init__(self, persist_directory: str = \"./chroma_db\", api_key: str = None):\n",
        "        self.persist_directory = persist_directory\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
        "        self.vectorstore = None\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[Document]:\n",
        "        \"\"\"Extract text content from PDF file\"\"\"\n",
        "        documents = []\n",
        "\n",
        "        try:\n",
        "            with open(pdf_path, 'rb') as file:\n",
        "                pdf_reader = pypdf.PdfReader(file)\n",
        "                total_pages = len(pdf_reader.pages)\n",
        "\n",
        "                for page_num in range(total_pages):\n",
        "                    page = pdf_reader.pages[page_num]\n",
        "                    text = page.extract_text()\n",
        "\n",
        "                    if text.strip():\n",
        "                        doc = Document(\n",
        "                            page_content=text,\n",
        "                            metadata={\n",
        "                                \"source\": Path(pdf_path).name,\n",
        "                                \"page\": page_num + 1,\n",
        "                                \"total_pages\": total_pages\n",
        "                            }\n",
        "                        )\n",
        "                        documents.append(doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path}: {str(e)}\")\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def process_directory(self, directory_path: str) -> List[Document]:\n",
        "        \"\"\"Process all PDF files in directory\"\"\"\n",
        "        all_documents = []\n",
        "        pdf_files = list(Path(directory_path).glob(\"*.pdf\"))\n",
        "\n",
        "        for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
        "            docs = self.extract_text_from_pdf(str(pdf_path))\n",
        "            all_documents.extend(docs)\n",
        "\n",
        "        return all_documents\n",
        "\n",
        "    def create_vector_store(self, documents: List[Document]) -> Chroma:\n",
        "        \"\"\"Create vector store from documents\"\"\"\n",
        "        chunks = self.text_splitter.split_documents(documents)\n",
        "\n",
        "        self.vectorstore = Chroma.from_documents(\n",
        "            documents=chunks,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=self.persist_directory\n",
        "        )\n",
        "\n",
        "        return self.vectorstore\n",
        "\n",
        "print(\"Document Processor class created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nS2NMuksE4-",
        "outputId": "4fe81ab6-b75b-4c23-9f74-e0d1cba4536c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Processor class created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import json\n",
        "\n",
        "class ComplianceResult:\n",
        "    \"\"\"Structure for compliance check results\"\"\"\n",
        "    def __init__(self, rule_id, rule_name, status, confidence, evidence, issues, recommendations, affected_sections):\n",
        "        self.rule_id = rule_id\n",
        "        self.rule_name = rule_name\n",
        "        self.status = status\n",
        "        self.confidence = confidence\n",
        "        self.evidence = evidence\n",
        "        self.issues = issues\n",
        "        self.recommendations = recommendations\n",
        "        self.affected_sections = affected_sections\n",
        "\n",
        "class ComplianceCheckerTool:\n",
        "    \"\"\"Custom tool for checking document compliance against rules\"\"\"\n",
        "\n",
        "    def __init__(self, vectorstore, api_key: str):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-pro\",\n",
        "            google_api_key=api_key,\n",
        "            temperature=0.1\n",
        "        )\n",
        "\n",
        "    def check_rule(self, rule: Dict, max_chunks: int = 8) -> ComplianceResult:\n",
        "        \"\"\"Execute compliance check for a rule\"\"\"\n",
        "\n",
        "        search_query = f\"{rule['name']} {' '.join(rule['keywords'][:5])}\"\n",
        "\n",
        "        relevant_docs = self.vectorstore.similarity_search(search_query, k=max_chunks)\n",
        "\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"[Section {i+1}] Source: {doc.metadata['source']}, Page: {doc.metadata['page']}\\n{doc.page_content}\"\n",
        "            for i, doc in enumerate(relevant_docs)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"You are a compliance expert analyzing corporate documents.\n",
        "\n",
        "RULE TO CHECK:\n",
        "ID: {rule['id']}\n",
        "Name: {rule['name']}\n",
        "Category: {rule['category']}\n",
        "Severity: {rule['severity']}\n",
        "Description: {rule['description']}\n",
        "\n",
        "Required Elements: {', '.join(rule['required_clauses'])}\n",
        "Keywords: {', '.join(rule['keywords'])}\n",
        "\n",
        "DOCUMENT SECTIONS:\n",
        "{context}\n",
        "\n",
        "TASK: Analyze if the documents comply with this rule.\n",
        "\n",
        "Provide your analysis in JSON format:\n",
        "{{\n",
        "    \"status\": \"COMPLIANT|NON_COMPLIANT|PARTIAL|UNCLEAR\",\n",
        "    \"confidence\": 0.0-1.0,\n",
        "    \"evidence\": [\"quote 1\", \"quote 2\"],\n",
        "    \"issues\": [\"issue 1 if any\"],\n",
        "    \"recommendations\": [\"suggestion 1\"],\n",
        "    \"reasoning\": \"brief explanation\"\n",
        "}}\n",
        "\n",
        "Be specific and cite exact phrases from the documents.\"\"\"\n",
        "\n",
        "        response = self.llm.invoke(prompt)\n",
        "\n",
        "        try:\n",
        "            json_start = response.content.find('{')\n",
        "            json_end = response.content.rfind('}') + 1\n",
        "            json_str = response.content[json_start:json_end]\n",
        "            analysis = json.loads(json_str)\n",
        "\n",
        "            affected_sections = [\n",
        "                {\n",
        "                    \"source\": doc.metadata['source'],\n",
        "                    \"page\": str(doc.metadata['page']),\n",
        "                    \"snippet\": doc.page_content[:150] + \"...\"\n",
        "                }\n",
        "                for doc in relevant_docs\n",
        "            ]\n",
        "\n",
        "            result = ComplianceResult(\n",
        "                rule_id=rule['id'],\n",
        "                rule_name=rule['name'],\n",
        "                status=analysis.get('status', 'UNCLEAR'),\n",
        "                confidence=float(analysis.get('confidence', 0.5)),\n",
        "                evidence=analysis.get('evidence', []),\n",
        "                issues=analysis.get('issues', []),\n",
        "                recommendations=analysis.get('recommendations', []),\n",
        "                affected_sections=affected_sections\n",
        "            )\n",
        "\n",
        "        except:\n",
        "            result = ComplianceResult(\n",
        "                rule_id=rule['id'],\n",
        "                rule_name=rule['name'],\n",
        "                status='UNCLEAR',\n",
        "                confidence=0.3,\n",
        "                evidence=[],\n",
        "                issues=['Analysis parsing error'],\n",
        "                recommendations=['Manual review required'],\n",
        "                affected_sections=[]\n",
        "            )\n",
        "\n",
        "        return result\n",
        "\n",
        "print(\"Compliance Checker Tool created\")"
      ],
      "metadata": {
        "id": "bVnfgG6C8bCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb395ae6-cf60-4b27-fc5b-f26fd9eb3504"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compliance Checker Tool created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFzOEBZOF4HR",
        "outputId": "2c3ffb57-6354-4521-8fba-0ed21c210d4c"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import pickle\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PROCESSING DOCUMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "documents = []\n",
        "for pdf_path in tqdm(list(Path('documents').glob('*.pdf')), desc=\"Reading PDFs\"):\n",
        "    reader = PdfReader(str(pdf_path))\n",
        "    for page_num, page in enumerate(reader.pages):\n",
        "        text = page.extract_text()\n",
        "        if text.strip():\n",
        "            documents.append(Document(page_content=text, metadata={'source': pdf_path.name, 'page': page_num + 1}))\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "class SimpleVectorStore:\n",
        "    def __init__(self, docs):\n",
        "        self.documents = docs\n",
        "        self.texts = [d.page_content.lower() for d in docs]\n",
        "\n",
        "    def similarity_search(self, query, k=4):\n",
        "        q_words = set(query.lower().split())\n",
        "        scores = [(len(q_words & set(t.split())), i) for i, t in enumerate(self.texts)]\n",
        "        scores.sort(reverse=True)\n",
        "        return [self.documents[i] for s, i in scores[:k] if s > 0]\n",
        "\n",
        "vectorstore = SimpleVectorStore(splits)\n",
        "with open('vectorstore.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorstore, f)\n",
        "\n",
        "print(f\"\\n✓ Done! {len(documents)} pages, {len(splits)} chunks\")\n",
        "print(\"✓ Ready for compliance audit!\\n\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PROCESSING DOCUMENTS\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading PDFs: 100%|██████████| 20/20 [00:05<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Done! 164 pages, 786 chunks\n",
            "✓ Ready for compliance audit!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class ComplianceResult:\n",
        "    rule_id: str\n",
        "    status: str\n",
        "    confidence: float\n",
        "    evidence: List[str]\n",
        "    issues: List[str]\n",
        "    recommendation: str\n",
        "\n",
        "class ComplianceCheckerTool:\n",
        "    def __init__(self, vectorstore, api_key):\n",
        "        self.vectorstore = vectorstore\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "    def check_rule(self, rule):\n",
        "        query = f\"{rule['name']} {rule['description']}\"\n",
        "        docs = self.vectorstore.similarity_search(query, k=5)\n",
        "\n",
        "        if not docs:\n",
        "            return ComplianceResult(rule['id'], 'UNCLEAR', 0.0, [], ['No relevant documents'], 'Upload documents')\n",
        "\n",
        "        context = \"\\n\\n\".join([f\"Doc: {d.metadata['source']}, Page {d.metadata['page']}\\n{d.page_content[:500]}\" for d in docs])\n",
        "\n",
        "        prompt = f\"\"\"Check compliance:\n",
        "\n",
        "RULE: {rule['name']}\n",
        "DESCRIPTION: {rule['description']}\n",
        "\n",
        "DOCUMENTS:\n",
        "{context}\n",
        "\n",
        "Respond:\n",
        "STATUS: [COMPLIANT/NON_COMPLIANT/PARTIAL/UNCLEAR]\n",
        "CONFIDENCE: [0-100]\n",
        "EVIDENCE: brief evidence\n",
        "ISSUES: any issues\n",
        "RECOMMENDATION: recommendation\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            text = response.text\n",
        "\n",
        "            status = 'UNCLEAR'\n",
        "            confidence = 0.5\n",
        "            evidence = []\n",
        "            issues = []\n",
        "            recommendation = ''\n",
        "\n",
        "            for line in text.split('\\n'):\n",
        "                if 'STATUS:' in line: status = line.split(':')[1].strip().split()[0]\n",
        "                elif 'CONFIDENCE:' in line:\n",
        "                    try: confidence = float(line.split(':')[1].strip().split()[0]) / 100\n",
        "                    except: pass\n",
        "                elif 'EVIDENCE:' in line: evidence.append(line.split(':', 1)[1].strip())\n",
        "                elif 'ISSUES:' in line: issues.append(line.split(':', 1)[1].strip())\n",
        "                elif 'RECOMMENDATION:' in line: recommendation = line.split(':', 1)[1].strip()\n",
        "\n",
        "            return ComplianceResult(rule['id'], status, confidence, evidence, issues, recommendation)\n",
        "        except Exception as e:\n",
        "            return ComplianceResult(rule['id'], 'UNCLEAR', 0.0, [], [f'Error: {str(e)}'], 'Manual review')\n",
        "\n",
        "print(\"✓ ComplianceCheckerTool \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMoOLOMUVNZK",
        "outputId": "db21564f-6d4e-4cac-87eb-1ea4225eb107"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ ComplianceCheckerTool \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open('compliance_rules.yaml', 'r') as f:\n",
        "    rules_data = yaml.safe_load(f)\n",
        "    rules = rules_data['rules']\n",
        "\n",
        "print(f\"Loaded {len(rules)} rules\\n\")\n",
        "\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPLIANCE AUDIT - SIMPLE VERSION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "def simple_check_rule(rule, vectorstore):\n",
        "    query = f\"{rule['name']} {rule['description']}\"\n",
        "    docs = vectorstore.similarity_search(query, k=5)\n",
        "\n",
        "    if not docs:\n",
        "        return {\n",
        "            'status': 'UNCLEAR',\n",
        "            'confidence': 0.0,\n",
        "            'evidence': 0,\n",
        "            'issues': ['No matching documents found'],\n",
        "            'recommendation': 'Add relevant policy documents'\n",
        "        }\n",
        "\n",
        "    keywords = rule['description'].lower().split()\n",
        "    evidence_count = sum(1 for doc in docs if any(kw in doc.page_content.lower() for kw in keywords))\n",
        "\n",
        "    if evidence_count >= 3:\n",
        "        status = 'COMPLIANT'\n",
        "        confidence = 0.8\n",
        "    elif evidence_count >= 1:\n",
        "        status = 'PARTIAL'\n",
        "        confidence = 0.5\n",
        "    else:\n",
        "        status = 'NON_COMPLIANT'\n",
        "        confidence = 0.6\n",
        "\n",
        "    return {\n",
        "        'status': status,\n",
        "        'confidence': confidence,\n",
        "        'evidence': evidence_count,\n",
        "        'issues': [] if status == 'COMPLIANT' else ['Insufficient evidence'],\n",
        "        'recommendation': 'Review found' if status == 'COMPLIANT' else 'Needs improvement'\n",
        "    }\n",
        "\n",
        "results = []\n",
        "for i, rule in enumerate(rules, 1):\n",
        "    print(f\"[{i}/{len(rules)}] {rule['id']} - {rule['name']}...\", end=\" \")\n",
        "\n",
        "    result = simple_check_rule(rule, vectorstore)\n",
        "\n",
        "    results.append({\n",
        "        'Rule ID': rule['id'],\n",
        "        'Rule Name': rule['name'],\n",
        "        'Category': rule['category'],\n",
        "        'Severity': rule['severity'],\n",
        "        'Status': result['status'],\n",
        "        'Confidence': f\"{result['confidence']:.0%}\",\n",
        "        'Evidence': result['evidence'],\n",
        "        'Recommendation': result['recommendation']\n",
        "    })\n",
        "\n",
        "    print(f\"{result['status']} ({result['confidence']:.0%})\")\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(df) > 0:\n",
        "    print(f\"\\n{df['Status'].value_counts()}\\n\")\n",
        "\n",
        "    print(\"CRITICAL ISSUES:\")\n",
        "    critical = df[(df['Severity'] == 'CRITICAL') & (df['Status'] != 'COMPLIANT')]\n",
        "    if len(critical) > 0:\n",
        "        print(critical[['Rule ID', 'Rule Name', 'Status']].to_string(index=False))\n",
        "    else:\n",
        "        print(\"None - All critical rules passed\")\n",
        "\n",
        "    # Save\n",
        "    filename = f\"compliance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"\\nReport saved: {filename}\\n\")\n",
        "    print(df.to_string(index=False))\n",
        "else:\n",
        "    print(\"ERROR: No results generated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6hJMZGBmpcn",
        "outputId": "5560bb5d-6e46-469c-c8bd-de777189616c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 15 rules\n",
            "\n",
            "======================================================================\n",
            "COMPLIANCE AUDIT - SIMPLE VERSION\n",
            "======================================================================\n",
            "\n",
            "[1/15] RULE_001 - Personal Data Encryption... COMPLIANT (80%)\n",
            "[2/15] RULE_002 - Data Retention Policy... COMPLIANT (80%)\n",
            "[3/15] RULE_003 - Multi-Factor Authentication... COMPLIANT (80%)\n",
            "[4/15] RULE_004 - Role-Based Access Control... COMPLIANT (80%)\n",
            "[5/15] RULE_005 - Breach Notification Timeline... COMPLIANT (80%)\n",
            "[6/15] RULE_006 - Incident Response Plan... COMPLIANT (80%)\n",
            "[7/15] RULE_007 - Background Verification... COMPLIANT (80%)\n",
            "[8/15] RULE_008 - Security Training... COMPLIANT (80%)\n",
            "[9/15] RULE_009 - Regular Security Audits... COMPLIANT (80%)\n",
            "[10/15] RULE_010 - Audit Log Retention... COMPLIANT (80%)\n",
            "[11/15] RULE_011 - Vendor Security Assessment... COMPLIANT (80%)\n",
            "[12/15] RULE_012 - Data Processing Agreements... COMPLIANT (80%)\n",
            "[13/15] RULE_013 - Data Subject Rights... COMPLIANT (80%)\n",
            "[14/15] RULE_014 - Privacy Notice... COMPLIANT (80%)\n",
            "[15/15] RULE_015 - Vulnerability Management... COMPLIANT (80%)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Status\n",
            "COMPLIANT    15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "CRITICAL ISSUES:\n",
            "None - All critical rules passed\n",
            "\n",
            "Report saved: compliance_report_20251202_011427.csv\n",
            "\n",
            " Rule ID                    Rule Name               Category Severity    Status Confidence  Evidence Recommendation\n",
            "RULE_001     Personal Data Encryption        Data Protection CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_002        Data Retention Policy        Data Protection     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_003  Multi-Factor Authentication         Access Control CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_004    Role-Based Access Control         Access Control     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_005 Breach Notification Timeline      Incident Response CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_006       Incident Response Plan      Incident Response     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_007      Background Verification    Employee Management   MEDIUM COMPLIANT        80%         5   Review found\n",
            "RULE_008            Security Training    Employee Management   MEDIUM COMPLIANT        80%         5   Review found\n",
            "RULE_009      Regular Security Audits     Audit & Compliance     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_010          Audit Log Retention     Audit & Compliance     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_011   Vendor Security Assessment Third-Party Management     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_012   Data Processing Agreements Third-Party Management CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_013          Data Subject Rights         Privacy Rights CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_014               Privacy Notice         Privacy Rights     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_015     Vulnerability Management        System Security CRITICAL COMPLIANT        80%         5   Review found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPLIANCE AUDIT - SIMPLE VERSION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Simple compliance checker without API\n",
        "def simple_check_rule(rule, vectorstore):\n",
        "    query = f\"{rule['name']} {rule['description']}\"\n",
        "    docs = vectorstore.similarity_search(query, k=5)\n",
        "\n",
        "    if not docs:\n",
        "        return {\n",
        "            'status': 'UNCLEAR',\n",
        "            'confidence': 0.0,\n",
        "            'evidence': 0,\n",
        "            'issues': ['No matching documents found'],\n",
        "            'recommendation': 'Add relevant policy documents'\n",
        "        }\n",
        "\n",
        "    # Simple keyword matching\n",
        "    keywords = rule['description'].lower().split()\n",
        "    evidence_count = sum(1 for doc in docs if any(kw in doc.page_content.lower() for kw in keywords))\n",
        "\n",
        "    if evidence_count >= 3:\n",
        "        status = 'COMPLIANT'\n",
        "        confidence = 0.8\n",
        "    elif evidence_count >= 1:\n",
        "        status = 'PARTIAL'\n",
        "        confidence = 0.5\n",
        "    else:\n",
        "        status = 'NON_COMPLIANT'\n",
        "        confidence = 0.6\n",
        "\n",
        "    return {\n",
        "        'status': status,\n",
        "        'confidence': confidence,\n",
        "        'evidence': evidence_count,\n",
        "        'issues': [] if status == 'COMPLIANT' else ['Insufficient evidence'],\n",
        "        'recommendation': 'Review found' if status == 'COMPLIANT' else 'Needs improvement'\n",
        "    }\n",
        "\n",
        "# Check all rules\n",
        "results = []\n",
        "for i, rule in enumerate(rules, 1):\n",
        "    print(f\"[{i}/{len(rules)}] {rule['id']} - {rule['name']}...\", end=\" \")\n",
        "\n",
        "    result = simple_check_rule(rule, vectorstore)\n",
        "\n",
        "    results.append({\n",
        "        'Rule ID': rule['id'],\n",
        "        'Rule Name': rule['name'],\n",
        "        'Category': rule['category'],\n",
        "        'Severity': rule['severity'],\n",
        "        'Status': result['status'],\n",
        "        'Confidence': f\"{result['confidence']:.0%}\",\n",
        "        'Evidence': result['evidence'],\n",
        "        'Recommendation': result['recommendation']\n",
        "    })\n",
        "\n",
        "    print(f\"{result['status']} ({result['confidence']:.0%})\")\n",
        "\n",
        "# Create report\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{df['Status'].value_counts()}\\n\")\n",
        "\n",
        "print(\"CRITICAL ISSUES:\")\n",
        "critical = df[(df['Severity'] == 'CRITICAL') & (df['Status'] != 'COMPLIANT')]\n",
        "print(critical[['Rule ID', 'Rule Name', 'Status']] if len(critical) > 0 else \"✓ None\")\n",
        "\n",
        "# Save\n",
        "filename = f\"compliance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "df.to_csv(filename, index=False)\n",
        "print(f\"\\n✓ Report saved: {filename}\\n\")\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76LoE7PeVytx",
        "outputId": "a744182e-21fd-4953-cec4-555b827179e6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "COMPLIANCE AUDIT - SIMPLE VERSION\n",
            "======================================================================\n",
            "\n",
            "[1/15] RULE_001 - Personal Data Encryption... COMPLIANT (80%)\n",
            "[2/15] RULE_002 - Data Retention Policy... COMPLIANT (80%)\n",
            "[3/15] RULE_003 - Multi-Factor Authentication... COMPLIANT (80%)\n",
            "[4/15] RULE_004 - Role-Based Access Control... COMPLIANT (80%)\n",
            "[5/15] RULE_005 - Breach Notification Timeline... COMPLIANT (80%)\n",
            "[6/15] RULE_006 - Incident Response Plan... COMPLIANT (80%)\n",
            "[7/15] RULE_007 - Background Verification... COMPLIANT (80%)\n",
            "[8/15] RULE_008 - Security Training... COMPLIANT (80%)\n",
            "[9/15] RULE_009 - Regular Security Audits... COMPLIANT (80%)\n",
            "[10/15] RULE_010 - Audit Log Retention... COMPLIANT (80%)\n",
            "[11/15] RULE_011 - Vendor Security Assessment... COMPLIANT (80%)\n",
            "[12/15] RULE_012 - Data Processing Agreements... COMPLIANT (80%)\n",
            "[13/15] RULE_013 - Data Subject Rights... COMPLIANT (80%)\n",
            "[14/15] RULE_014 - Privacy Notice... COMPLIANT (80%)\n",
            "[15/15] RULE_015 - Vulnerability Management... COMPLIANT (80%)\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Status\n",
            "COMPLIANT    15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "CRITICAL ISSUES:\n",
            "✓ None\n",
            "\n",
            "✓ Report saved: compliance_report_20251202_011429.csv\n",
            "\n",
            " Rule ID                    Rule Name               Category Severity    Status Confidence  Evidence Recommendation\n",
            "RULE_001     Personal Data Encryption        Data Protection CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_002        Data Retention Policy        Data Protection     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_003  Multi-Factor Authentication         Access Control CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_004    Role-Based Access Control         Access Control     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_005 Breach Notification Timeline      Incident Response CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_006       Incident Response Plan      Incident Response     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_007      Background Verification    Employee Management   MEDIUM COMPLIANT        80%         5   Review found\n",
            "RULE_008            Security Training    Employee Management   MEDIUM COMPLIANT        80%         5   Review found\n",
            "RULE_009      Regular Security Audits     Audit & Compliance     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_010          Audit Log Retention     Audit & Compliance     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_011   Vendor Security Assessment Third-Party Management     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_012   Data Processing Agreements Third-Party Management CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_013          Data Subject Rights         Privacy Rights CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_014               Privacy Notice         Privacy Rights     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_015     Vulnerability Management        System Security CRITICAL COMPLIANT        80%         5   Review found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPLIANCE REPORT - DETAILED RESULTS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\" SUMMARY BY STATUS\")\n",
        "print(\"-\" * 70)\n",
        "print(df['Status'].value_counts())\n",
        "\n",
        "print(\"\\n SUMMARY BY CATEGORY\")\n",
        "print(\"-\" * 70)\n",
        "print(df.groupby('Category')['Status'].value_counts())\n",
        "\n",
        "print(\"\\n CRITICAL ISSUES\")\n",
        "print(\"-\" * 70)\n",
        "critical = df[(df['Severity'] == 'CRITICAL') & (df['Status'] != 'COMPLIANT')]\n",
        "if len(critical) > 0:\n",
        "    print(critical[['Rule ID', 'Rule Name', 'Status']].to_string(index=False))\n",
        "else:\n",
        "    print(\"✓ No critical issues found\")\n",
        "\n",
        "print(\"\\n HIGH SEVERITY ISSUES\")\n",
        "print(\"-\" * 70)\n",
        "high = df[(df['Severity'] == 'HIGH') & (df['Status'] != 'COMPLIANT')]\n",
        "if len(high) > 0:\n",
        "    print(high[['Rule ID', 'Rule Name', 'Status']].to_string(index=False))\n",
        "else:\n",
        "    print(\"✓ No high severity issues found\")\n",
        "\n",
        "filename = f\"compliance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "df.to_csv(filename, index=False)\n",
        "\n",
        "print(f\"\\n✓ Full report saved to: {filename}\\n\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FULL COMPLIANCE REPORT\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "total = len(df)\n",
        "compliant = len(df[df['Status'] == 'COMPLIANT'])\n",
        "partial = len(df[df['Status'] == 'PARTIAL'])\n",
        "non_compliant = len(df[df['Status'] == 'NON_COMPLIANT'])\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(f\"COMPLIANCE RATE: {compliant}/{total} ({compliant/total*100:.1f}%)\")\n",
        "print(f\"✓ Compliant: {compliant} | ◐ Partial: {partial} | ✗ Non-Compliant: {non_compliant}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKCTpF8OTGv1",
        "outputId": "68b4157e-9a84-45e9-be3d-1895da287f6f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "COMPLIANCE REPORT - DETAILED RESULTS\n",
            "======================================================================\n",
            "\n",
            " SUMMARY BY STATUS\n",
            "----------------------------------------------------------------------\n",
            "Status\n",
            "COMPLIANT    15\n",
            "Name: count, dtype: int64\n",
            "\n",
            " SUMMARY BY CATEGORY\n",
            "----------------------------------------------------------------------\n",
            "Category                Status   \n",
            "Access Control          COMPLIANT    2\n",
            "Audit & Compliance      COMPLIANT    2\n",
            "Data Protection         COMPLIANT    2\n",
            "Employee Management     COMPLIANT    2\n",
            "Incident Response       COMPLIANT    2\n",
            "Privacy Rights          COMPLIANT    2\n",
            "System Security         COMPLIANT    1\n",
            "Third-Party Management  COMPLIANT    2\n",
            "Name: count, dtype: int64\n",
            "\n",
            " CRITICAL ISSUES\n",
            "----------------------------------------------------------------------\n",
            "✓ No critical issues found\n",
            "\n",
            " HIGH SEVERITY ISSUES\n",
            "----------------------------------------------------------------------\n",
            "✓ No high severity issues found\n",
            "\n",
            "✓ Full report saved to: compliance_report_20251202_011431.csv\n",
            "\n",
            "======================================================================\n",
            "FULL COMPLIANCE REPORT\n",
            "======================================================================\n",
            "\n",
            " Rule ID                    Rule Name               Category Severity    Status Confidence  Evidence Recommendation\n",
            "RULE_001     Personal Data Encryption        Data Protection CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_002        Data Retention Policy        Data Protection     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_003  Multi-Factor Authentication         Access Control CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_004    Role-Based Access Control         Access Control     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_005 Breach Notification Timeline      Incident Response CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_006       Incident Response Plan      Incident Response     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_007      Background Verification    Employee Management   MEDIUM COMPLIANT        80%         5   Review found\n",
            "RULE_008            Security Training    Employee Management   MEDIUM COMPLIANT        80%         5   Review found\n",
            "RULE_009      Regular Security Audits     Audit & Compliance     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_010          Audit Log Retention     Audit & Compliance     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_011   Vendor Security Assessment Third-Party Management     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_012   Data Processing Agreements Third-Party Management CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_013          Data Subject Rights         Privacy Rights CRITICAL COMPLIANT        80%         5   Review found\n",
            "RULE_014               Privacy Notice         Privacy Rights     HIGH COMPLIANT        80%         5   Review found\n",
            "RULE_015     Vulnerability Management        System Security CRITICAL COMPLIANT        80%         5   Review found\n",
            "\n",
            "======================================================================\n",
            "COMPLIANCE RATE: 15/15 (100.0%)\n",
            "✓ Compliant: 15 | ◐ Partial: 0 | ✗ Non-Compliant: 0\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "json_data = []\n",
        "for result in results:\n",
        "    json_data.append({\n",
        "        'rule_id': result['Rule ID'],\n",
        "        'rule_name': result['Rule Name'],\n",
        "        'category': result['Category'],\n",
        "        'severity': result['Severity'],\n",
        "        'status': result['Status'],\n",
        "        'confidence': result['Confidence'],\n",
        "        'evidence_count': result['Evidence'],\n",
        "        'recommendation': result['Recommendation']\n",
        "    })\n",
        "\n",
        "json_filename = f\"compliance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "with open(json_filename, 'w') as f:\n",
        "    json.dump({\n",
        "        'audit_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'total_rules': len(results),\n",
        "        'compliant': len([r for r in results if r['Status'] == 'COMPLIANT']),\n",
        "        'partial': len([r for r in results if r['Status'] == 'PARTIAL']),\n",
        "        'non_compliant': len([r for r in results if r['Status'] == 'NON_COMPLIANT']),\n",
        "        'compliance_rate': f\"{len([r for r in results if r['Status'] == 'COMPLIANT'])/len(results)*100:.1f}%\",\n",
        "        'results': json_data\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"✓ JSON report saved: {json_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA5oglnFTIH-",
        "outputId": "16a562c5-42cf-4af9-85db-ba775fe3c929"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ JSON report saved: compliance_report_20251202_011433.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"AGENT WORKFLOW - MULTI-STEP COMPLIANCE QUESTIONING\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "class SimpleComplianceAgent:\n",
        "    def __init__(self, vectorstore):\n",
        "        self.vectorstore = vectorstore\n",
        "\n",
        "    def ask(self, question):\n",
        "        \"\"\"Answer compliance questions using document search\"\"\"\n",
        "        docs = self.vectorstore.similarity_search(question, k=5)\n",
        "\n",
        "        if not docs:\n",
        "            return {\n",
        "                'answer': 'No relevant information found',\n",
        "                'evidence': [],\n",
        "                'confidence': 'Low'\n",
        "            }\n",
        "\n",
        "        evidence = []\n",
        "        for doc in docs:\n",
        "            evidence.append({\n",
        "                'source': doc.metadata['source'],\n",
        "                'page': doc.metadata['page'],\n",
        "                'content': doc.page_content[:200] + \"...\"\n",
        "            })\n",
        "\n",
        "        if len(docs) >= 3:\n",
        "            answer = f\"Yes, found {len(docs)} relevant policy sections addressing this requirement.\"\n",
        "            confidence = 'High'\n",
        "        elif len(docs) >= 1:\n",
        "            answer = f\"Partially - found {len(docs)} relevant sections, but may need more coverage.\"\n",
        "            confidence = 'Medium'\n",
        "        else:\n",
        "            answer = \"Insufficient evidence found in policy documents.\"\n",
        "            confidence = 'Low'\n",
        "\n",
        "        return {\n",
        "            'answer': answer,\n",
        "            'evidence': evidence,\n",
        "            'confidence': confidence\n",
        "        }\n",
        "\n",
        "agent = SimpleComplianceAgent(vectorstore)\n",
        "\n",
        "questions = [\n",
        "    \"Is personal data encrypted according to our policies?\",\n",
        "    \"What are the data retention requirements?\",\n",
        "    \"Are there breach notification procedures defined?\",\n",
        "    \"Do we have multi-factor authentication requirements?\",\n",
        "    \"What background check procedures exist for employees?\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, question in enumerate(questions, 1):\n",
        "    print(f\"\\n[Question {i}] {question}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    result = agent.ask(question)\n",
        "    results.append({'question': question, **result})\n",
        "\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Confidence: {result['confidence']}\")\n",
        "    print(f\"Evidence: {len(result['evidence'])} documents found\")\n",
        "\n",
        "    if result['evidence']:\n",
        "        print(\"\\nTop Evidence:\")\n",
        "        for j, ev in enumerate(result['evidence'][:2], 1):\n",
        "            print(f\"  {j}. {ev['source']} (Page {ev['page']})\")\n",
        "            print(f\"     {ev['content'][:150]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"AGENT WORKFLOW COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import json\n",
        "with open('agent_qa_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\n✓ Q&A results saved to: agent_qa_results.json\")\n",
        "print(f\"✓ Processed {len(questions)} multi-step compliance questions\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcz-sFqyXZ3o",
        "outputId": "d1e84967-9bd9-45b3-d020-7085d811b366"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "AGENT WORKFLOW - MULTI-STEP COMPLIANCE QUESTIONING\n",
            "======================================================================\n",
            "\n",
            "\n",
            "[Question 1] Is personal data encrypted according to our policies?\n",
            "----------------------------------------------------------------------\n",
            "Answer: Yes, found 5 relevant policy sections addressing this requirement.\n",
            "Confidence: High\n",
            "Evidence: 5 documents found\n",
            "\n",
            "Top Evidence:\n",
            "  1. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT - Escrow Agreement.pdf (Page 2)\n",
            "     12 March 1998                                      National Computing Centre\n",
            "                                                   Oxford House, Oxford R...\n",
            "  2. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT - Escrow Agreement_23.pdf (Page 2)\n",
            "     12 March 1998                                      National Computing Centre\n",
            "                                                   Oxford House, Oxford R...\n",
            "\n",
            "[Question 2] What are the data retention requirements?\n",
            "----------------------------------------------------------------------\n",
            "Answer: Yes, found 5 relevant policy sections addressing this requirement.\n",
            "Confidence: High\n",
            "Evidence: 5 documents found\n",
            "\n",
            "Top Evidence:\n",
            "  1. FEDERATEDGOVERNMENTINCOMESECURITIESINC_04_28_2020-EX-99.SERV AGREE-SERVICES AGREEMENT.pdf (Page 3)\n",
            "     EXHIBIT A\n",
            "DESCRIPTION OF SERVICES\n",
            "The following are the categories of Services to be provided by FASC to the Adviser pursuant to the Agreement:\n",
            "Perfor...\n",
            "  2. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT (1)_20.pdf (Page 21)\n",
            "     a brief description of the journal, the frequency of publication,\n",
            "            and pricing information.\n",
            "      4.    Purchasing procedure: The user deci...\n",
            "\n",
            "[Question 3] Are there breach notification procedures defined?\n",
            "----------------------------------------------------------------------\n",
            "Answer: Yes, found 5 relevant policy sections addressing this requirement.\n",
            "Confidence: High\n",
            "Evidence: 5 documents found\n",
            "\n",
            "Top Evidence:\n",
            "  1. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT (1)_20.pdf (Page 23)\n",
            "     Subscriber Features\n",
            "- --------------------------------------------------------------------------------\n",
            "User Access to their Custom Page\n",
            "      Another ...\n",
            "  2. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT (1)_20.pdf (Page 20)\n",
            "     Articles are provided to the user in both PDF and HTML format\n",
            "- --------------------------------------------------------------------------------\n",
            "Purch...\n",
            "\n",
            "[Question 4] Do we have multi-factor authentication requirements?\n",
            "----------------------------------------------------------------------\n",
            "Answer: Yes, found 5 relevant policy sections addressing this requirement.\n",
            "Confidence: High\n",
            "Evidence: 5 documents found\n",
            "\n",
            "Top Evidence:\n",
            "  1. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT (1)_20.pdf (Page 20)\n",
            "     In this scenario, a registered user requests to see a document to which\n",
            "      they do not currently have access. For example, they may be viewing an\n",
            " ...\n",
            "  2. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT (1)_20.pdf (Page 20)\n",
            "     Articles are provided to the user in both PDF and HTML format\n",
            "- --------------------------------------------------------------------------------\n",
            "Purch...\n",
            "\n",
            "[Question 5] What background check procedures exist for employees?\n",
            "----------------------------------------------------------------------\n",
            "Answer: Yes, found 5 relevant policy sections addressing this requirement.\n",
            "Confidence: High\n",
            "Evidence: 5 documents found\n",
            "\n",
            "Top Evidence:\n",
            "  1. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT - Escrow Agreement.pdf (Page 8)\n",
            "     4     Documentation describing the procedures for building / compiling /\n",
            "            executing / using the software (technical notes, user guides).\n",
            "  ...\n",
            "  2. HEALTHGATEDATACORP_11_24_1999-EX-10.1-HOSTING AND MANAGEMENT AGREEMENT - Escrow Agreement.pdf (Page 8)\n",
            "     despatch to the party to which such notice is required to be given\n",
            "            or made under this Agreement addressed to the principal place of\n",
            "      ...\n",
            "\n",
            "======================================================================\n",
            "AGENT WORKFLOW COMPLETE\n",
            "======================================================================\n",
            "\n",
            "✓ Q&A results saved to: agent_qa_results.json\n",
            "✓ Processed 5 multi-step compliance questions\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas pyyaml plotly\n",
        "\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"COMPLIANCE AUDIT SYSTEM\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    with open('vectorstore.pkl', 'rb') as f:\n",
        "        vectorstore = pickle.load(f)\n",
        "    print(\"✓ Vector store loaded\")\n",
        "except:\n",
        "    print(\"✗ Vector store not found\")\n",
        "    vectorstore = None\n",
        "\n",
        "try:\n",
        "    with open('compliance_rules.yaml', 'r') as f:\n",
        "        rules = yaml.safe_load(f)['rules']\n",
        "    print(f\"✓ Loaded {len(rules)} rules\")\n",
        "except:\n",
        "    print(\"✗ Rules file not found\")\n",
        "    rules = []\n",
        "\n",
        "if vectorstore and rules:\n",
        "    print(\"\\nRunning audit...\\n\")\n",
        "    results = []\n",
        "\n",
        "    for i, rule in enumerate(rules):\n",
        "        print(f\"Checking {i+1}/{len(rules)}: {rule['name']}\")\n",
        "        query = f\"{rule['name']} {rule['description']}\"\n",
        "        docs = vectorstore.similarity_search(query, k=5)\n",
        "        evidence = len(docs)\n",
        "\n",
        "        status = 'COMPLIANT' if evidence >= 3 else 'PARTIAL' if evidence >= 1 else 'NON_COMPLIANT'\n",
        "\n",
        "        results.append({\n",
        "            'Rule ID': rule['id'],\n",
        "            'Rule Name': rule['name'],\n",
        "            'Status': status,\n",
        "            'Evidence': evidence\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"AUDIT RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Compliant: {len(df[df['Status']=='COMPLIANT'])}\")\n",
        "    print(f\"Partial: {len(df[df['Status']=='PARTIAL'])}\")\n",
        "    print(f\"Non-Compliant: {len(df[df['Status']=='NON_COMPLIANT'])}\")\n",
        "    print(\"\\n\")\n",
        "    display(df)\n",
        "\n",
        "    df.to_csv('audit_report.csv', index=False)\n",
        "    print(\"\\n✓ Report saved as 'audit_report.csv'\")\n",
        "else:\n",
        "    print(\"\\n Cannot run audit. Upload vectorstore.pkl and compliance_rules.yaml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FrXrB0lca1Db",
        "outputId": "87a0968c-ee86-4717-e786-eaa8ef3594da"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "==================================================\n",
            "COMPLIANCE AUDIT SYSTEM\n",
            "==================================================\n",
            "✓ Vector store loaded\n",
            "✓ Loaded 15 rules\n",
            "\n",
            "Running audit...\n",
            "\n",
            "Checking 1/15: Personal Data Encryption\n",
            "Checking 2/15: Data Retention Policy\n",
            "Checking 3/15: Multi-Factor Authentication\n",
            "Checking 4/15: Role-Based Access Control\n",
            "Checking 5/15: Breach Notification Timeline\n",
            "Checking 6/15: Incident Response Plan\n",
            "Checking 7/15: Background Verification\n",
            "Checking 8/15: Security Training\n",
            "Checking 9/15: Regular Security Audits\n",
            "Checking 10/15: Audit Log Retention\n",
            "Checking 11/15: Vendor Security Assessment\n",
            "Checking 12/15: Data Processing Agreements\n",
            "Checking 13/15: Data Subject Rights\n",
            "Checking 14/15: Privacy Notice\n",
            "Checking 15/15: Vulnerability Management\n",
            "\n",
            "==================================================\n",
            "AUDIT RESULTS\n",
            "==================================================\n",
            "Compliant: 15\n",
            "Partial: 0\n",
            "Non-Compliant: 0\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     Rule ID                     Rule Name     Status  Evidence\n",
              "0   RULE_001      Personal Data Encryption  COMPLIANT         5\n",
              "1   RULE_002         Data Retention Policy  COMPLIANT         5\n",
              "2   RULE_003   Multi-Factor Authentication  COMPLIANT         5\n",
              "3   RULE_004     Role-Based Access Control  COMPLIANT         5\n",
              "4   RULE_005  Breach Notification Timeline  COMPLIANT         5\n",
              "5   RULE_006        Incident Response Plan  COMPLIANT         5\n",
              "6   RULE_007       Background Verification  COMPLIANT         5\n",
              "7   RULE_008             Security Training  COMPLIANT         5\n",
              "8   RULE_009       Regular Security Audits  COMPLIANT         5\n",
              "9   RULE_010           Audit Log Retention  COMPLIANT         5\n",
              "10  RULE_011    Vendor Security Assessment  COMPLIANT         5\n",
              "11  RULE_012    Data Processing Agreements  COMPLIANT         5\n",
              "12  RULE_013           Data Subject Rights  COMPLIANT         5\n",
              "13  RULE_014                Privacy Notice  COMPLIANT         5\n",
              "14  RULE_015      Vulnerability Management  COMPLIANT         5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-053ffc19-3dcb-4049-a9cd-d30d9a50b70e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rule ID</th>\n",
              "      <th>Rule Name</th>\n",
              "      <th>Status</th>\n",
              "      <th>Evidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RULE_001</td>\n",
              "      <td>Personal Data Encryption</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RULE_002</td>\n",
              "      <td>Data Retention Policy</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RULE_003</td>\n",
              "      <td>Multi-Factor Authentication</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RULE_004</td>\n",
              "      <td>Role-Based Access Control</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RULE_005</td>\n",
              "      <td>Breach Notification Timeline</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RULE_006</td>\n",
              "      <td>Incident Response Plan</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>RULE_007</td>\n",
              "      <td>Background Verification</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>RULE_008</td>\n",
              "      <td>Security Training</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RULE_009</td>\n",
              "      <td>Regular Security Audits</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RULE_010</td>\n",
              "      <td>Audit Log Retention</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>RULE_011</td>\n",
              "      <td>Vendor Security Assessment</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>RULE_012</td>\n",
              "      <td>Data Processing Agreements</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>RULE_013</td>\n",
              "      <td>Data Subject Rights</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>RULE_014</td>\n",
              "      <td>Privacy Notice</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>RULE_015</td>\n",
              "      <td>Vulnerability Management</td>\n",
              "      <td>COMPLIANT</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-053ffc19-3dcb-4049-a9cd-d30d9a50b70e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-053ffc19-3dcb-4049-a9cd-d30d9a50b70e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-053ffc19-3dcb-4049-a9cd-d30d9a50b70e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-80e7c70d-f6f2-47d9-8c16-234594f233c6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-80e7c70d-f6f2-47d9-8c16-234594f233c6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-80e7c70d-f6f2-47d9-8c16-234594f233c6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_94bbccee-6493-40be-939b-1221e673f52c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_94bbccee-6493-40be-939b-1221e673f52c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Rule ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"RULE_010\",\n          \"RULE_012\",\n          \"RULE_001\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rule Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Audit Log Retention\",\n          \"Data Processing Agreements\",\n          \"Personal Data Encryption\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"COMPLIANT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Evidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Report saved as 'audit_report.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import plotly.express as px\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "import pypdf\n",
        "import tempfile\n",
        "\n",
        "st.set_page_config(page_title=\"Compliance Checker\", layout=\"wide\")\n",
        "\n",
        "if \"results_df\" not in st.session_state:\n",
        "    st.session_state.results_df = None\n",
        "\n",
        "st.title(\"Policy Compliance Checker\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Configuration\")\n",
        "    api_key = st.text_input(\"Google API Key\", type=\"password\")\n",
        "    uploaded_files = st.file_uploader(\"Upload PDFs\", type=[\"pdf\"], accept_multiple_files=True)\n",
        "    run_audit = st.button(\"Run Audit\", type=\"primary\", use_container_width=True)\n",
        "\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "col1.metric(\"Total Rules\", \"15\")\n",
        "col2.metric(\"Documents\", len(uploaded_files) if uploaded_files else 0)\n",
        "\n",
        "if st.session_state.results_df is not None:\n",
        "    df = st.session_state.results_df\n",
        "    compliant = len(df[df[\"Status\"] == \"COMPLIANT\"])\n",
        "    col3.metric(\"Compliance Rate\", f\"{compliant/len(df)*100:.1f}%\")\n",
        "    col4.metric(\"Issues\", len(df[df[\"Status\"] == \"NON_COMPLIANT\"]))\n",
        "else:\n",
        "    col3.metric(\"Compliance Rate\", \"--\")\n",
        "    col4.metric(\"Issues\", \"--\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "\n",
        "tab1, tab2 = st.tabs([\"Results\", \"Analytics\"])\n",
        "\n",
        "with tab1:\n",
        "    if st.session_state.results_df is not None:\n",
        "        st.dataframe(st.session_state.results_df, use_container_width=True, hide_index=True)\n",
        "        csv = st.session_state.results_df.to_csv(index=False).encode(\"utf-8\")\n",
        "        st.download_button(\"Download Report\", csv, f\"report_{datetime.now().strftime('%Y%m%d')}.csv\")\n",
        "    else:\n",
        "        st.info(\"Upload documents and run audit to see results\")\n",
        "\n",
        "with tab2:\n",
        "    if st.session_state.results_df is not None:\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            category_data = st.session_state.results_df.groupby([\"Category\", \"Status\"]).size().reset_index(name=\"Count\")\n",
        "            fig = px.bar(category_data, x=\"Category\", y=\"Count\", color=\"Status\", title=\"By Category\")\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "        with col2:\n",
        "            severity_data = st.session_state.results_df.groupby([\"Severity\", \"Status\"]).size().reset_index(name=\"Count\")\n",
        "            fig = px.bar(severity_data, x=\"Severity\", y=\"Count\", color=\"Status\", title=\"By Severity\")\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"Run audit to see analytics\")\n",
        "\n",
        "if run_audit:\n",
        "    if not api_key:\n",
        "        st.error(\"Enter API key\")\n",
        "    elif not uploaded_files:\n",
        "        st.error(\"Upload PDFs\")\n",
        "    else:\n",
        "        with st.spinner(\"Processing...\"):\n",
        "            try:\n",
        "                os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "                with tempfile.TemporaryDirectory() as temp_dir:\n",
        "                    documents = []\n",
        "                    for uploaded_file in uploaded_files:\n",
        "                        temp_path = os.path.join(temp_dir, uploaded_file.name)\n",
        "                        with open(temp_path, \"wb\") as f:\n",
        "                            f.write(uploaded_file.getbuffer())\n",
        "\n",
        "                        with open(temp_path, \"rb\") as file:\n",
        "                            pdf_reader = pypdf.PdfReader(file)\n",
        "                            for page_num, page in enumerate(pdf_reader.pages):\n",
        "                                text = page.extract_text()\n",
        "                                if text.strip():\n",
        "                                    documents.append(Document(\n",
        "                                        page_content=text,\n",
        "                                        metadata={\"source\": uploaded_file.name, \"page\": page_num + 1}\n",
        "                                    ))\n",
        "\n",
        "                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "                    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "                    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "                    vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=temp_dir)\n",
        "\n",
        "                    rules = [\n",
        "                        {\"id\": \"RULE_001\", \"category\": \"Data Protection\", \"name\": \"Personal Data Encryption\", \"description\": \"encrypted data rest transit\", \"severity\": \"CRITICAL\"},\n",
        "                        {\"id\": \"RULE_002\", \"category\": \"Data Protection\", \"name\": \"Data Retention Policy\", \"description\": \"retention periods years\", \"severity\": \"HIGH\"},\n",
        "                        {\"id\": \"RULE_003\", \"category\": \"Access Control\", \"name\": \"Multi-Factor Authentication\", \"description\": \"MFA mandatory users\", \"severity\": \"CRITICAL\"},\n",
        "                        {\"id\": \"RULE_004\", \"category\": \"Access Control\", \"name\": \"Role-Based Access Control\", \"description\": \"RBAC least privilege\", \"severity\": \"HIGH\"},\n",
        "                        {\"id\": \"RULE_005\", \"category\": \"Incident Response\", \"name\": \"Breach Notification\", \"description\": \"breaches reported 72 hours\", \"severity\": \"CRITICAL\"},\n",
        "                        {\"id\": \"RULE_006\", \"category\": \"Incident Response\", \"name\": \"Response Plan\", \"description\": \"incident response procedures\", \"severity\": \"HIGH\"},\n",
        "                        {\"id\": \"RULE_007\", \"category\": \"Employee Management\", \"name\": \"Background Verification\", \"description\": \"background checks employees\", \"severity\": \"MEDIUM\"},\n",
        "                        {\"id\": \"RULE_008\", \"category\": \"Employee Management\", \"name\": \"Security Training\", \"description\": \"annual training employees\", \"severity\": \"MEDIUM\"},\n",
        "                        {\"id\": \"RULE_009\", \"category\": \"Audit and Compliance\", \"name\": \"Security Audits\", \"description\": \"audits annually\", \"severity\": \"HIGH\"},\n",
        "                        {\"id\": \"RULE_010\", \"category\": \"Audit and Compliance\", \"name\": \"Log Retention\", \"description\": \"logs retained 12 months\", \"severity\": \"HIGH\"},\n",
        "                        {\"id\": \"RULE_011\", \"category\": \"Third-Party\", \"name\": \"Vendor Assessment\", \"description\": \"vendors security assessment\", \"severity\": \"HIGH\"},\n",
        "                        {\"id\": \"RULE_012\", \"category\": \"Third-Party\", \"name\": \"Data Processing Agreements\", \"description\": \"DPAs third parties\", \"severity\": \"CRITICAL\"},\n",
        "                        {\"id\": \"RULE_013\", \"category\": \"Privacy Rights\", \"name\": \"Data Subject Rights\", \"description\": \"data access deletion 30 days\", \"severity\": \"CRITICAL\"},\n",
        "                        {\"id\": \"RULE_014\", \"category\": \"Privacy Rights\", \"name\": \"Privacy Notice\", \"description\": \"privacy notice collection\", \"severity\": \"HIGH\"},\n",
        "                        {\"id\": \"RULE_015\", \"category\": \"System Security\", \"name\": \"Vulnerability Management\", \"description\": \"vulnerabilities patched 30 days\", \"severity\": \"CRITICAL\"}\n",
        "                    ]\n",
        "\n",
        "                    results = []\n",
        "                    progress = st.progress(0)\n",
        "\n",
        "                    for i, rule in enumerate(rules):\n",
        "                        docs = vectorstore.similarity_search(rule[\"description\"], k=5)\n",
        "                        keywords = rule[\"description\"].split()\n",
        "                        evidence = sum(1 for doc in docs if any(kw in doc.page_content.lower() for kw in keywords))\n",
        "\n",
        "                        status = \"COMPLIANT\" if evidence >= 3 else \"PARTIAL\" if evidence >= 1 else \"NON_COMPLIANT\"\n",
        "                        confidence = 0.8 if evidence >= 3 else 0.5 if evidence >= 1 else 0.3\n",
        "\n",
        "                        results.append({\n",
        "                            \"Rule ID\": rule[\"id\"],\n",
        "                            \"Rule Name\": rule[\"name\"],\n",
        "                            \"Category\": rule[\"category\"],\n",
        "                            \"Severity\": rule[\"severity\"],\n",
        "                            \"Status\": status,\n",
        "                            \"Confidence\": f\"{confidence:.0%}\",\n",
        "                            \"Evidence\": evidence\n",
        "                        })\n",
        "                        progress.progress((i + 1) / len(rules))\n",
        "\n",
        "                    st.session_state.results_df = pd.DataFrame(results)\n",
        "                    progress.empty()\n",
        "                    st.success(\"Audit complete!\")\n",
        "                    st.rerun()\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error: {str(e)}\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"<div style='text-align: center; color: gray;'>Compliance Checker v1.0</div>\", unsafe_allow_html=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3zMAuVopd6a",
        "outputId": "410a7128-ee5d-457e-8b4a-011c699b7525"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "requirements = '''streamlit\n",
        "langchain==0.1.0\n",
        "langchain-google-genai==0.0.6\n",
        "langchain-community==0.0.13\n",
        "chromadb==0.4.22\n",
        "pypdf==3.17.4\n",
        "pandas\n",
        "plotly\n",
        "pyyaml'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERo68_8Jc5On",
        "outputId": "70131b8b-1832-4650-891e-774eb994d4cf"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}